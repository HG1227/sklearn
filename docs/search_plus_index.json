{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Update time： 2020-08-04 "},"Chapter1/":{"url":"Chapter1/","title":"常用模型","keywords":"","body":"常用模型 Update time： 2020-05-23 "},"Chapter1/决策树.html":{"url":"Chapter1/决策树.html","title":"决策树","keywords":"","body":"决策树 决策树（Decision Tree）是一种非参数的有监督学习方法，它能够从一系列有特征和标签的数据中总结出决策规 则，并用树状图的结构来呈现这些规则，以解决分类和回归问题 。 决策树算法的核心是要解决两个问题： 1）如何从数据表中找出最佳节点和最佳分枝？ 2）如何让决策树停止生长，防止过拟合？ sklearn中的决策树 模块 sklearn.tree sklearn 中决策树的类都在 ”tree“ 这个模块之下。这个模块总共包含五个类 tree.DecisionTreeClassifier 分类树 tree.DecisionTreeRegressor 回归树 tree.export_graphviz 将生成的决策树导出为DOT格式，画图专用 tree.ExtraTreeClassifier 高随机版本的分类树 tree.ExtraTreeRegressor 高随机版本的回归树 DecisionTreeClassifier class sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0 ) 重要参数 criterion 为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标 叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好 。 不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是 说，在同一棵决策树上，叶子节点的不纯度一定是最低的 。 Criterion这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择 1）输入”entropy“，使用信息熵（Entropy ) 2）输入”gini“，使用基尼系数（Gini Impurity ) 比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是在实际使用中，信息熵和基尼系数的效果基 本相同。信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数。另外，因为信息熵对不纯度更加敏 感，所以信息熵作为指标时，决策树的生长会更加“精细”，因此对于高维数据或者噪音很多的数据，信息熵很容易 过拟合，基尼系数在这种情况下效果往往比较好。当模型拟合程度不足的时候，即当模型在训练集和测试集上都表 现不太好的时候，使用信息熵 。 参数 criterion 如何影响模型? 确定不纯度的计算方法，帮忙找出最佳节点和最佳分枝，不纯度越低，决策树对训练集 的拟合越好 可能的输入有哪 些？ 不填默认基尼系数，填写gini使用基尼系数，填写entropy使用信息增益 怎样选取参数？ 通常就使用基尼系数 数据维度很大，噪音很大时使用基尼系数 维度低，数据比较清晰的时候，信息熵和基尼系数没区别 当决策树的拟合程度不够的时候，使用信息熵 两个都试试，不好就换另外一个 random_state & splitter random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据 （比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来 splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best\"，决策树在分枝时虽然随机，但是还是会 优先选择更重要的特征进行分枝（重要性可以通过属性featureimportances查看），输入“random\"，决策树在 分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这 也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能 性。当然，树一旦建成，依然是使用剪枝参数来防止过拟合 。 剪枝参数 在不加限制的情况下，一棵决策树会生长到衡量不纯度的指标最优，或者没有更多的特征可用为止。这样的决策树 往往会过拟合，这就是说，它会在训练集上表现很好，在测试集上却表现糟糕。我们收集的样本数据不可能和整体 的状况完全一致，因此当一棵决策树对训练数据有了过于优秀的解释性，它找出的规则必然包含了训练样本中的噪 声，并使它对未知数据的拟合程度不足 。 为了让决策树有更好的泛化性，我们要对决策树进行剪枝。剪枝策略对决策树的影响巨大，正确的剪枝策略是优化 决策树算法的核心。sklearn为我们提供了不同的剪枝策略 。 max_depth 限制树的最大深度，超过设定深度的树枝全部剪掉 这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所 以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效再决定是否增加设定深度 。 min_samples_leaf & min_samples_split min_samples_leaf 限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分 枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生 一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引 起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很 大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题 中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择 。 min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生 。 max_features & min_impurity_decrease max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工， max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量 而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型 学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。 min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本中更新的 功能，在0.19版本之前时使用min_impurity_split。 目标权重参数 class_weight & min_weight_fraction_leaf 完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要 判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不 做，全把结果预测成“否”，正确率也能有99%。因此我们要使用 class_weight 参数对样本标签进行一定的均衡，给 少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给 与数据集中的所有标签相同的权重。 有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_ weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_ fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分 重要属性和接口 featureimportances 属性是在模型训练之后，能够调用查看的模型的各种性质。对决策树来说，最重要的是featureimportances，能够查看各个特征对模型的重要性 apply和predict sklearn中许多算法的接口都是相似的，比如说我们之前已经用到的fit和score，几乎对每个算法都可以使用。除了 这两个接口之外，决策树最常用的接口还有apply和predict。apply中输入测试集返回每个测试样本所在的叶子节 点的索引，predict输入测试集返回每个测试样本的标签 。 所有接口中要求输入X_train和X_test的部分，输入的特征矩阵必须至少是一个二维矩阵。sklearn不接受任何一维矩阵作为特征矩阵被输入 。 DecisionTreeRegressor class sklearn.tree.DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0 ) 几乎所有参数，属性及接口都和分类树一模一样。需要注意的是，在回归树种，没有标签分布是否均衡的问题，因 此没有class_weight这样的参数 。 重要参数，属性及接口 criterion 回归树衡量分枝质量的指标，支持的标准有三种 ： 1）输入 \"mse\" 使用均方误差 mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化 L2 损失 2）输入 “friedman_mse” 使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差 3）输入 \"mae\" 使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化 L1 损失 属性中最重要的依然是featureimportances，接口依然是apply, fit, predict, score最核心。 MSE=1N∑i=1N(fi−yi)2\r \r M S E=\\frac{1}{N} \\sum_{i=1}^{N}\\left(f_{i}-y_{i}\\right)^{2}\r\r MSE=​N​​1​​∑​i=1​N​​(f​i​​−y​i​​)​2​​ 其中N是样本数量，i是每一个数据样本，fififi 是模型回归出的数值，yiyiyi 是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。 然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下： R2=1−uv\r R^{2}=1-\\frac{u}{v}\r R​2​​=1−​v​​u​​ u=∑i=1N(fi−yi)2v=∑i=1N(yi−y^)2\r u=\\sum_{i=1}^{N}\\left(f_{i}-y_{i}\\right)^{2} \\quad v=\\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}\\right)^{2}\r u=∑​i=1​N​​(f​i​​−y​i​​)​2​​v=∑​i=1​N​​(y​i​​−​y​^​​)​2​​ 其中 uuu 是残差平方和（MSE * N），vvv 是总平方和，N 是样本数量，iii 是每一个数据样本，fififi 是模型回归出的数值，yiyiyi 是样本点i实际的数值标签。y^\\hat y​y​^​​ 是真实数值标签的平均数。R 平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正 值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误 差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是 neg_mean_squared_error 去掉负号的数字 。 决策树的优缺点 决策树优点 易于理解和解释，因为树木可以画出来被看见 需要很少的数据准备。其他很多算法通常都需要数据规范化，需要创建虚拟变量并删除空值等。但请注意， sklearn 中的决策树模块不支持对缺失值的处理。 使用树的成本（比如说，在预测数据的时候）是用于训练树的数据点的数量的对数，相比于其他算法，这是 一个很低的成本。 能够同时处理数字和分类数据，既可以做回归又可以做分类。其他技术通常专门用于分析仅具有一种变量类 型的数据集。 能够处理多输出问题，即含有多个标签的问题，注意与一个标签中含有多种标签分类的问题区别开 是一个白盒模型，结果很容易能够被解释。如果在模型中可以观察到给定的情况，则可以通过布尔逻辑轻松 解释条件。相反，在黑盒模型中（例如，在人工神经网络中），结果可能更难以解释。 可以使用统计测试验证模型，这让我们可以考虑模型的可靠性。 即使其假设在某种程度上违反了生成数据的真实模型，也能够表现良好 决策树的缺点 决策树学习者可能创建过于复杂的树，这些树不能很好地推广数据。这称为过度拟合。修剪，设置叶节点所 需的最小样本数或设置树的最大深度等机制是避免此问题所必需的，而这些参数的整合和调整对初学者来说 会比较晦涩 决策树可能不稳定，数据中微小的变化可能导致生成完全不同的树，这个问题需要通过集成算法来解决。 决策树的学习是基于贪婪算法，它靠优化局部最优（每个节点的最优）来试图达到整体的最优，但这种做法 不能保证返回全局最优决策树。这个问题也可以由集成算法来解决，在随机森林中，特征和样本会在分枝过 程中被随机采样。 有些概念很难学习，因为决策树不容易表达它们，例如XOR，奇偶校验或多路复用器问题。 如果标签中的某些类占主导地位，决策树学习者会创建偏向主导类的树。因此，建议在拟合决策树之前平衡 数据集 附录 分类树参数列表 分类树属性列表 分类树接口列表 Update time： 2020-05-23 "},"Chapter1/随机森林.html":{"url":"Chapter1/随机森林.html","title":"随机森林","keywords":"","body":"随机森林 集成算法概述 集成学习（ensemble learning）是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通 过在数据上构建多个模型，集成所有模型的建模结果。基本上所有的机器学习领域都可以看到集成学习的身影，在 现实中集成学习也有相当大的作用，它可以用来做市场营销模拟的建模，统计客户来源，保留和流失，也可用来预 测疾病的风险和病患者的易感性。在现在的各种算法竞赛中，随机森林，梯度提升树（GBDT），Xgboost等集成 算法的身影也随处可见，可见其效果之好，应用之广 集成算法的目标 集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或 分类表现 . 多个模型集成成为的模型叫做集成评估器（ensemble estimator），组成集成评估器的每个模型都叫做基评估器 （base estimator）。通常来说，有三类集成算法：装袋法（Bagging），提升法（Boosting）和stacking 装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结 果。装袋法的代表模型就是随机森林。 提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本 进行预测，从而构成一个强评估器。提升法的代表模型有 Adaboost 和梯度提升树 sklearn中的随机森林 sklearn 中的集成算法模块 ensemble 类 类的功能 ensemble.RandomForestClassifier 随机森林分类 ensemble.RandomForestRegressor 随机森林回归 决策树的核心问题有两个，一个是如何找出正确的特征来进行提问，即如何分枝，二是树生长到什么时候应该停下 于第一个问题，我们定义了用来衡量分枝质量的指标不纯度，分类树的不纯度用基尼系数或信息熵来衡量，回归 树的不纯度用MSE均方误差来衡量。每次分枝时，决策树对所有的特征进行不纯度计算，选取不纯度最低的特征进行分枝，分枝后，又再对被分枝的不同取值下，计算每个特征的不纯度，继续选取不纯度最低的特征进行分枝。 RandomForestClassifier class sklearn.ensemble.RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None ) 重要参数 控制基评估器的参数 参数 含义 criterion 不纯度的衡量指标，有基尼系数和信息熵两种选择 max_depth 树的最大深度，超过最大深度的树枝都会被剪掉 min_samples_leaf 一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样 本，否则分枝就不会发生 min_samples_split 一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分 枝，否则分枝就不会发生 max_features max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃， 默认值为总特征个数开平方取整 min_impurity_decrease 限制信息增益的大小，信息增益小于设定数值的分枝不会发生 n_estimators 这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators越 大，模型的效果往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡 n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为 100。这个修正显示出了使用者的调参倾向：要更大的n_estimators 。 random_state 随机森林的本质是一种装袋集成算法（bagging），装袋集成算法是对基评估器的预测结果进行平均或用多数表决 原则来决定集成评估器的结果 。 建立了25棵树，对任何一个样本而言，平均或多数表决原则下，当且仅当有13棵以上的树判断错误的时候，随机森林才会判断错误。单独一棵决策树对红酒数据集的分类准确率在0.85上下浮动，假设一棵树判断错误的可能性为0.2(ε)，那20棵树以上都判断错误的可能性是 erandomforest=∑i=1325C25iεi(1−ε)25−i=0.000369\r e_{\\text {random}_{\\text {forest}}}=\\sum_{i=13}^{25} C_{25}^{i} \\varepsilon^{i}(1-\\varepsilon)^{25-i}=0.000369\r e​random​forest​​​​=∑​i=13​25​​C​25​i​​ε​i​​(1−ε)​25−i​​=0.000369 其中，iii 是判断错误的次数，也是判错的树的数量，ε是一棵树判断错误的概率，（1-ε）是判断正确的概率，共判对25-i次 . 随机森林中其实也有random_state，用法和分类树中相似，只不过在分类树中，一个random_state只控制生成一 棵树，而随机森林中的random_state控制的是生成森林的模式，而非让一个森林中只有一棵树 。 当random_state固定时，随机森林中生成是一组固定的树，但每棵树依然是不一致的，这是用”随机挑选特征进行分枝“的方法得到的随机性。并且我们可以证明，当这种随机性越大的时候，袋装法的效果一般会越来越好。用袋装法集成时，基分类器应当是相互独立的，是不相同的 bootstrap & oob_score 要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而袋装法正是通过有放回的 随机抽样技术来形成不同的训练数据，bootstrap 就是用来控制抽样技术的参数 bootstrap参数默认True，代表采用这种有放回的随机抽样技术。通常，这个参数不会被我们设置为False 然而有放回抽样也会有自己的问题。由于是有放回，一些样本可能在同一个自助集中出现多次，而其他一些却可能 被忽略，一般来说，自助集大约平均会包含63%的原始数据。因为每一个样本被抽到某个自助集中的概率为 1−(1−1n)n\r 1-\\left(1-\\frac{1}{n}\\right)^{n}\r 1−(1−​n​​1​​)​n​​ 当n足够大时，这个概率收敛于1-(1/e)，约等于0.632。因此，会有约37%的训练数据被浪费掉，没有参与建模，这些数据被称为袋外数据(out of bag data，简写为oob)。除了我们最开始就划分好的测试集之外，这些数据也可以被用来作为集成算法的测试集。也就是说，在使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据来测试我们的模型即可 当然，这也不是绝对的，当n和n_estimators都不够大的时候，很可能就没有数据掉落在袋外，自然也就无法使用 oob 数据来测试模型了 如果希望用袋外数据来测试，则需要在实例化时就将oobscore这个参数调整为True，训练完毕之后，我们可以用随机森林的另一个重要属性：oob_score来查看我们的在袋外数据上测试的结果 重要属性和接口 estimators_ 查看森林中树的状况 rfc = RandomForestClassifier(n_estimators=20,random_state=2) rfc = rfc.fit(Xtrain, Ytrain) #随机森林的重要属性之一：estimators，查看森林中树的状况 rfc.estimators_[0].random_state for i in range(len(rfc.estimators_)): print(rfc.estimators_[i].random_state) oobscore 来查看我们的在袋外数据上测试的结果 #无需划分训练集和测试集 rfc = RandomForestClassifier(n_estimators=25,oob_score=True) rfc = rfc.fit(wine.data,wine.target) #重要属性oob_score_ rfc.oob_score_ featureimportances ... predict_proba 随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score。除此之外，还需要注随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的 predict_proba 返回的概率，得到一个平均概率，从而决定测试样本的分类 rfc = RandomForestClassifier(n_estimators=25) rfc = rfc.fit(Xtrain, Ytrain) rfc.score(Xtest,Ytest) rfc.feature_importances_ rfc.apply(Xtest) rfc.predict(Xtest) rfc.predict_proba(Xtest) RandomForestRegressor class sklearn.ensemble.RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None ) 所有的参数，属性与接口，全部和随机森林分类器一致。仅有的不同就是回归树与分类树的不同，不纯度的指标，参数Criterion不一致 。 重要参数 criterion 1）输入\"mse\"使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为 特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失 2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差 3）输入\"mae\"使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失 MSE=1N∑i=1N(fi−yi)2\r M S E=\\frac{1}{N} \\sum_{i=1}^{N}\\left(f_{i}-y_{i}\\right)^{2}\r MSE=​N​​1​​∑​i=1​N​​(f​i​​−y​i​​)​2​​ 其中N是样本数量，iii 是每一个数据样本，fif_if​i​​ 是模型回归出的数值，yiy_iy​i​​ 是样本点i实际的数值标签。所以 MSE 的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE 不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好 然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 R2=1−uv\r R^{2}=1-\\frac{u}{v}\r R​2​​=1−​v​​u​​ u=∑i=1N(fi−yi)2v=∑i=1N(yi−y^)2\r u=\\sum_{i=1}^{N}\\left(f_{i}-y_{i}\\right)^{2} \\quad v=\\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}\\right)^{2}\r u=∑​i=1​N​​(f​i​​−y​i​​)​2​​v=∑​i=1​N​​(y​i​​−​y​^​​)​2​​ 其中 uuu 是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fif_if​i​​ 是模型回归出的数值，yiy_iy​i​​ 是样本点i实际的数值标签。y^\\hat y​y​^​​ 是真实数值标签的平均数。R 平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。 值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字 重要属性和接口 重要的属性和接口，都与随机森林的分类器相一致，还是apply, fit, predict和score最为核心。值得一提的是，随机森林回归并没有 predict_proba 这个接口，因为对于回归来说，并不存在一个样本要被分到某个类别的概率问题，因此没有 predict_proba 这个接口 n_estimators from sklearn.datasets import load_boston from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor boston = load_boston() regressor = RandomForestRegressor(n_estimators=100,random_state=0) cross_val_score(regressor, boston.data, boston.target, cv=10 ,scoring = \"neg_mean_squared_error\") sorted(sklearn.metrics.SCORERS.keys()) 返回十次交叉验证的结果，注意在这里，如果不填写scoring = \"neg_mean_squared_error\"，交叉验证默认的模型衡量指标是R平方，因此交叉验证的结果可能有正也可能有负。而如果写上scoring，则衡量标准是负MSE，交叉验证的结果只可能为负. 附录 RFC的参数列表 RFC的属性列表 RFC的接口列表 Update time： 2020-05-23 "},"Chapter1/主成分分析PCA.html":{"url":"Chapter1/主成分分析PCA.html","title":"主成分分析PCA","keywords":"","body":"主成分分析PCA 维度指的是样本的数量或特征的数量，一般无特别说明，指的都是特征的数 量 。 降维算法中的”降维“，指的是降低特征矩阵中特征的数量。 PCA 在降维过程中，我们会减少特征的数量，这意味着删除数据，数据量变少则表示模型可以获取的信息会变少，模型的表现可能会因此受影响。同时，在高维数据中，必然有一些特征是不带有有效的信息的（比如噪音），或者有一些特征带有的信息和其他一些特征是重复的（比如一些特征可能会线性相关）。我们希望能够找出一种办法来帮助我们衡量特征上所带的信息量，让我们在降维的过程中，能够即减少特征的数量，又保留大部分有效信息——将那些带有重复信息的特征合并，并删除那些带无效信息的特征等等——逐渐创造出能够代表原特征矩阵大部分信息的，特征更少的，新特征矩阵 。 在降维中，PCA 使用的信息量衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息量越多。 Var=1n−1∑i=1n(xi−x^)2\r Var=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\hat{x}\\right)^{2}\r Var=​n−1​​1​​∑​i=1​n​​(x​i​​−​x​^​​)​2​​ Var代表一个特征的方差，n代表样本量，xi代表一个特征中的每个样本取值，xhat代表这一列样本的均值。 class sklearn.decomposition.PCA (n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0, iterated_power=’auto’, random_state=None ) 思考：PCA和特征选择技术都是特征工程的一部分，它们有什么不同？ 特征工程中有三种方式：特征提取，特征创造和特征选择。仔细观察上面的降维例子和上周我们讲解过的特征 选择，你发现有什么不同了吗? 特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特 征在原数据的哪个位置，代表着原数据上的什么含义。 而 PCA，是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某 些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓 PCA 都建立了怎样的新特征向 量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而 来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了。以 PCA 为代表的降维算法因此是 特征创造（feature creation，或feature construction）的一种。 可以想见，PCA 一般不适用于探索特征和标签之间的关系的模型（如线性回归），因为无法解释的新特征和标 签之间的关系不具有意义。在线性回归模型中，我们使用特征选择 重要参数 n_components n_components 是我们降维后需要的维度，即降维后需要保留的特征数量 。 选择最好的n_components：累积可解释方差贡献率曲线 。 当参数n_components中不填写任何值，则默认返回min(X.shape)个特征，一般来说，样本量都会大于特征数目，所以什么都不填就相当于转换了新特征空间，但没有减少特征的个数。一般来说，不会使用这种输入方式。但我们却可以使用这种输入方式来画出累计可解释方差贡献率曲线，以此选择最好的n_components的整数取值。累积可解释方差贡献率曲线是一条以降维后保留的特征个数为横坐标，降维后新特征矩阵捕捉到的可解释方差贡献率为纵坐标的曲线，能够帮助我们决定n_components最好的取值。 import numpy as np pca_line = PCA().fit(X) plt.plot([1,2,3,4],np.cumsum(pca_line.explained_variance_ratio_)) plt.xticks([1,2,3,4]) #这是为了限制坐标轴显示为整数 plt.xlabel(\"number of components after dimension reduction\") plt.ylabel(\"cumulative explained variance ratio\") plt.show() 最大似然估计自选超参数 除了输入整数，n_components还有哪些选择呢？数学大神Minka, T.P.在麻省理工学院媒体实验室做研究时找出了让PCA用最大似然估计(maximum likelihood estimation)自选超参数的方法，输入mle 作为n_components的参数输入，就可以调用这种方法 pca_mle = PCA(n_components=\"mle\") pca_mle = pca_mle.fit(X) X_mle = pca_mle.transform(X) 按信息量占比选超参数 输入[0,1]之间的浮点数，并且让参数 svd_solver =='full'，表示希望降维后的总解释性方差占比大于n_components指定的百分比，即是说，希望保留百分之多少的信息量。比如说，如果我们希望保留97%的信息量，就可以输入n_components = 0.97，PCA会自动选出能够让保留的信息量超过97%的特征数量。 pca_f = PCA(n_components=0.97,svd_solver=\"full\") pca_f = pca_f.fit(X) X_f = pca_f.transform(X) pca_f.explained_variance_ratio_ svd_solver 与 random_state 参数svd_solver是在降维过程中，用来控制矩阵分解的一些细节的参数。有四种模式可选：auto, full, arpack,randomized，默认”auto\"。 通常我们就选用auto，不必对这个参数纠结太多 重要属性 components_ explained_variance_ explained_variance_ratio_ PCA是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓PCA都建立了怎样的新特征向量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了 但是其实，在矩阵分解时，PCA是有目标的：在原有特征的基础上，找出能够让信息尽量聚集的新特征向量。在sklearn使用的PCA和SVD联合的降维方法中，这些新特征向量组成的新特征空间其实就是V(k,n)。当V(k,n)是数字时，我们无法判断V(k,n)和原有的特征究竟有着怎样千丝万缕的数学联系。但是，如果原特征矩阵是图像，V(k,n)这个空间矩阵也可以被可视化的话，我们就可以通过两张图来比较，就可以看出新特征空间究竟从原始数据里提取了什么重要的信息 重要接口 inverse_transform 接口inverse_transform，可以将我们归一化，标准化，甚至做过哑变量的特征矩阵还原回原始数据中的特征矩阵 , 这几乎在向我们暗示，任何有inverse_transform这个接口的过程都是可逆的。PCA 应该也是如此。 PCA 参数列表 PCA 属性列表 PCA 接口列表 Update time： 2020-05-23 "},"Chapter1/逻辑回归.html":{"url":"Chapter1/逻辑回归.html","title":"逻辑回归","keywords":"","body":"逻辑回归 理解逻辑回归，必须要有一定的数学基础，必须理解损失函数，正则化，梯度下降，海森矩阵等等这些复杂的概念，才能够对逻辑回归进行调优 概率，研究的是自变量和因变量之间的关系 似然，研究的是参数取值与因变量之间的关系 sklearn 中的逻辑回归 逻辑回归相关的类 说明 linear_model.LogisticRegression 逻辑回归分类器（又叫logit回归，最大熵分类器） linear_model.LogisticRegressionCV 带交叉验证的逻辑回归分类器 linear_model.logistic_regression_path 计算Logistic回归模型以获得正则化参数的列表 linear_model.SGDClassifier 利用梯度下降求解的线性分类器（SVM，逻辑回归等等） linear_model.SGDRegressor 利用梯度下降最小化正则化后的损失函数的线性回归模型 1. linear_model.LogisticRegression class sklearn.linear_model.LogisticRegression (penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None ) 参数： L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小）， 参数 θ\\thetaθ 的取值会逐渐变小，但L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0 在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，所以L1正则化本质是一个特征选择的过程，掌管了参数的“稀疏性”。L1正 则化越强，参数向量中就越多的参数为0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。因此，如果特征量很大，数据维度很高，我们会倾向于使用L1正则化 ,相对的，L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数会非常接近于0。通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化。 属性列表: 接口列表 Update time： 2020-05-23 "},"Chapter1/聚类算法K-Means.html":{"url":"Chapter1/聚类算法K-Means.html","title":"聚类算法K-Means","keywords":"","body":"聚类算法K-Means 1. KMeans是如何工作的 顺序 过程 1 随机抽取K个样本作为最初的质心 2 开始循环： 2.1 将每个样本点分配到离他们最近的质心，生成K个簇 2.2 对于每个簇，计算所有被分到该簇的样本点的平均值作为新的质心 3 当质心的位置不再发生变化，迭代停止，聚类完成 2. sklearn.cluster.KMeans class sklearn.cluster.KMeans (n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, tol=0.0001, precompute_distances=’auto’, verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=’auto’ ) 对于一个簇来说，所有样本点到质心的距离之和越小，我们就认为这个簇中的样本越相似，簇内差异就越小。而距离的衡量方法有多种，令 xxx 表示簇中的一个样本点，μ\\muμ 表示该簇中的质心，n表示每个样本点中的特征数目，i表示组成点 的每个特征，则该样本点到质心的距离可以由以下距离来度量：欧几里得距离d(x,μ)=∑i=1n(xi−μi)2\r \\text {欧几里得距离} d(x, \\mu)=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{i}\\right)^{2}}\r 欧几里得距离d(x,μ)=√​∑​i=1​n​​(x​i​​−μ​i​​)​2​​​​​ 曼哈顿距离d(x,μ)=∑i=1n(∣xi−μ∣)\r \\text {曼哈顿距离}d(x, \\mu)=\\sum_{i=1}^{n}\\left(\\left|x_{i}-\\mu\\right|\\right)\r 曼哈顿距离d(x,μ)=∑​i=1​n​​(∣x​i​​−μ∣) 余弦距离cosθ=∑1n(xi∗μ)∑1n(xi)2∗∑1n(μ)2\r \\text {余弦距离} \\cos \\theta=\\frac{\\sum_{1}^{n}\\left(x_{i} * \\mu\\right)}{\\sqrt{\\sum_{1}^{n}\\left(x_{i}\\right)^{2}} * \\sqrt{\\sum_{1}^{n}(\\mu)^{2}}}\r 余弦距离cosθ=​√​∑​1​n​​(x​i​​)​2​​​​​∗√​∑​1​n​​(μ)​2​​​​​​​∑​1​n​​(x​i​​∗μ)​​ 采用欧几里得距离，则一个簇中所有样本点到质心的距离的平方和为： Cluster Sum of Square (CSS)=∑j=0m∑i=1n(xi−μi)2\r \\text { Cluster Sum of Square }(C S S)=\\sum_{j=0}^{m} \\sum_{i=1}^{n}\\left(x_{i}-\\mu_{i}\\right)^{2}\r  Cluster Sum of Square (CSS)=∑​j=0​m​​∑​i=1​n​​(x​i​​−μ​i​​)​2​​ TotalCluster Sum of Square=∑l=1kCSSl\r \\text {TotalCluster Sum of Square}=\\sum_{l=1}^{k} C S S_{l}\r TotalCluster Sum of Square=∑​l=1​k​​CSS​l​​ 其中，m 为一个簇中样本的个数，j 是每个样本的编号。这个公式被称为簇内平方和（cluster Sum of Square），叫做 Inertia。而将一个数据集中的所有簇的簇内平方和相加，就得到了整体平方和（Total Cluster Sum of Square），又叫做 total inertia。Total Inertia 越小，代表着每个簇内样本越相似，聚类的效果就越好。因此 KMeans 追求的是，求解能够让 Inertia 最小化的质心 3. 重要参数 n_clusters n_clusters是KMeans中的k，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认为8类，但通常我们的聚类结果会是一个小于8的结果 init & random_state & n_init 在K-Means中有一个重要的环节，就是放置初始质心。如果有足够的时间，K-means一定会收敛，但Inertia可能收敛到局部最小值。是否能够收敛到真正的最小值很大程度上取决于质心的初始化。init就是用来帮助我们决定初始化方式的参数。 初始质心放置的位置不同，聚类的结果很可能也会不一样，一个好的质心选择可以让K-Means避免更多的计算，让算法收敛稳定且更快。在之前讲解初始质心的放置时，我们是使用”随机“的方法在样本点中抽取k个样本作为初始质心，这种方法显然不符合”稳定且更快“的需求。为此，我们可以使用random_state参数来控制每次生成的初始质心都在相同位置，甚至可以画学习曲线来确定最优的random_state是哪个整数。 一个random_state对应一个质心随机初始化的随机数种子。如果不指定随机数种子，则sklearn中的K-means并不会只选择一个随机模式扔出结果，而会在每个随机数种子下运行多次，并使用结果最好的一个随机数种子来作为初始质心。我们可以使用参数n_init来选择，每个随机数种子下运行的次数。这个参数不常用到，默认10次，如果我们希望运行的结果更加精确，那我们可以增加这个参数n_init的值来增加每个随机数种子下运行的次数。为了优化选择初始质心的方法 , 在sklearn中，我们使用参数 init ='k-means ++'来选择使用k-means ++作为质心初始化的方案 init：可输入\"k-means++\"，\"random\"或者一个n维数组。这是初始化质心的方法，默认\"k-means++\"。输入\"kmeans++\"：一种为K均值聚类选择初始聚类中心的聪明的办法，以加速收敛。如果输入了n维数组，数组的形状应该是(n_clusters，n_features)并给出初始质心 random_state：控制每次质心随机初始化的随机数种子 n_init：整数，默认10，使用不同的质心随机初始化的种子来运行 k-means 算法的次数。最终结果会是基于 Inertia 来计算的 n_init 次连续运行后的最佳输出 max_iter & tol：让迭代停下来 当质心不再移动，Kmeans算法就会停下来。但在完全收敛之前，我们也可以使用max_iter，最大迭代次数，或者tol，两次迭代间Inertia下降的量，这两个参数来让迭代提前停下来。 max_iter：整数，默认300，单次运行的k-means算法的最大迭代次数 tol：浮点数，默认1e-4，两次迭代间Inertia下降的量，如果两次迭代之间Inertia下降的值小于tol所设定的值，迭代就会停下 4. 聚类算法的模型评估指标 4.1 当真实标签未知的时候：轮廓系数 在99%的情况下，我们是对没有真实标签的数据进行探索，也就是对不知道真正答案的数据进行聚类。这样的聚 类，是完全依赖于评价簇内的稠密程度（簇内差异小）和簇间的离散程度（簇外差异大）来评估聚类的效果。其中轮廓系数是最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量： 1）样本与其自身所在的簇中的其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离 2）样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中的所有点之间的平均距离 根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好 单个样本的轮廓系数计算为：s=b−amax(a,b)\r s=\\frac{b-a}{\\max (a, b)}\r s=​max(a,b)​​b−a​​ 公式可以被解析为：s={1−a/b, if ab0, if a=bb/a−1, if a>b\r s=\\left\\{\\begin{array}{ll}\r {1-a / b,} & {\\text { if } ab}\r \\end{array}\\right.\r s=​⎩​⎨​⎧​​​1−a/b,​0,​b/a−1,​​​ if ab​ if a=b​ if a>b​​ 很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。可以总结为轮廓系数越接近于1越好，负数则表示聚类效果非常差。 在 sklearn 中，我们使用模块metrics中的类silhouette_score来计算轮廓系数，它返回的是一个数据集中，所有样本的轮廓系数的均值。但我们还有同在metrics模块中的silhouette_sample，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数。 from sklearn.metrics import silhouette_score from sklearn.metrics import silhouette_samples X y_pred silhouette_score(X,y_pred) silhouette_score(X,cluster_.labels_) silhouette_samples(X,y_pred) 4.2 当真实标签未知的时候：Calinski-Harabaz Index 除了轮廓系数是最常用的，还有卡林斯基-哈拉巴斯指数（Calinski-Harabaz Index，简称CHI，也被称为方差比标准），戴维斯-布尔丁指数（Davies-Bouldin）以及权变矩阵（Contingency Matrix）可以使用 标签未知时的评估指标 卡林斯基-哈拉巴斯指数 sklearn.metrics.calinski_harabaz_score (X, y_pred) 戴维斯-布尔丁指数 sklearn.metrics.davies_bouldin_score (X, y_pred) 权变矩阵 sklearn.metrics.cluster.contingency_matrix (X, y_pred) from sklearn.metrics import calinski_harabaz_score X y_pred calinski_harabaz_score(X, y_pred) 5. KMeans参数列表 6. KMeans属性列表 7. KMeans接口列表 Update time： 2020-05-23 "},"Chapter1/支持向量机.html":{"url":"Chapter1/支持向量机.html","title":"支持向量机","keywords":"","body":"支持向量机 1. sklearn.svm.SVC 参数、属性和接口 class sklearn.svm.SVC (C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None ) 参数 C ：浮点数，默认1，必须大于等于0，可不填 松弛系数的惩罚项系数。如果 C 值设定比较大，那 SVC 可能会选择边际较小的，能够更好地分类所有训 练点的决策边界，不过模型的训练时间也会更长。如果 C 的设定值较小，那 SVC 会尽量最大化边界，决 策功能会更简单，但代价是训练的准确度。换句话说，C 在 SVM 中的影响就像正则化参数对逻辑回归的 影响 kernel ：核函数，默认是 rbf，可以是 linear , poly , rbf, sigmoid, precomputed degree ：整数，可不填，默认3, 多项式核函数的次数（poly），如果核函数没有选择poly，这个参数会被忽略 gamma ： 浮点数，可不填，默认auto 核函数的系数，仅在参数Kernel的选项为rbf,poly和sigmoid的时候有效 输入auto，自动使用1/(n_features)作为gamma的取值 输入scale，则使用1/(n_features * X.std())作为gamma的取值 输入auto_deprecated，则表示没有传递明确的gamma值（不推荐使用） coef0 ：浮点数，可不填，默认=0.0 核函数中的常数项，它只在参数kernel为poly和sigmoid的时候有效 选取与核函数相关的参数：degree & gamma & coef0 主要调节的参数有：C 、kernel 、degree 、gamma 、coef0 。 clf = SVC(kernel = \"linear\") 重要属性 : SVC的接口列表 ： 2. 二分类SVC中的样本不均衡问题：重要参数class_weight 首先，分类模型天生会倾向于多数的类，让多数类更容易被判断正确，少数类被牺牲掉。因为对于模型而言，样本量越大的标签可以学习的信息越多，算法就会更加依赖于从多数类中学到的信息来进行判断。如果我们希望捕获少数类，模型就会失败。其次，模型评估指标会失去意义。这种分类状况下，即便模型什么也不做，全把所有人都当成不会犯罪的人，准确率也能非常高，这使得模型评估指标accuracy变得毫无意义，根本无法达到我们的“要识别出会犯罪的人”的建模目的。 所以现在，我们首先要让算法意识到数据的标签是不均衡的，通过施加一些惩罚或者改变样本本身，来让模型向着捕获少数类的方向建模。然后，我们要改进我们的模型评估指标，使用更加针对于少数类的指标来优化模型 在支持向量机中，我们要大力依赖我们调节样本均衡的参数：SVC类中的class_weight和接口fit中可以设定的sample_weight. 2.1 SVC 的参数：class_weight 可输入字典或者balanced，可不填，默认None 对SVC，将类 i 的参数 C 设置为class_weight [i] * C。 如果没有给出具体的 class_weight，则所有类都被假设为占有相同的权重1，模型会根据数据原本的状况去训练。如果希望改善样本不均衡状况，请输入形如{\"标签的值1\"：权重1，\"标签的值2\"：权重2}的字典，则参数C将会自动被设为：标签的值1的C：权重1 C，标签的值2的C：权重2C 或者，可以使用“balanced”模式，这个模式使用y的值自动调整与输入数据中的类频率成反比的权重为 n_samples/(n_classes * np.bincount(y)) 2.2 SVC 的接口fit 的参数：sample_weight 数组，结构为 (n_samples )，必须对应输入fit中的特征矩阵的每个样本 每个样本在 fit 时的权重，让权重 每个样本对应的C值来迫使分类器强调设定的权重更大的样本。通常，较大的权重加在少数类的样本上，以迫使模型向着少数类的方向建模 通常来说，这两个参数我们只选取一个来设置。如果我们同时设置了两个参数，则C会同时受到两个参数的影响，即 class_weight 中设定的权重 sample_weight中设定的权重 C 3. 重要参数probability，接口predict_proba以及decision_function 我们在SVM中利用超平面来判断我们的样本，本质上来说，当两个点的距离是相同的符号的时候，越远离超平面的样本点归属于某个标签类的概率就很大。比如说，一个距离超平面0.1的点，和一个距离超平面100的点，明显是距离为0.1的点更有可能是负类别的点混入了边界。同理，一个距离超平面距离为-0.1的点，和一个离超平面距离为-100的点，明显是-100的点的标签更有可能是负类。所以，到超平面的距离一定程度上反应了样本归属于某个标签类的可能性。接口decision_function返回的值也因此被我们认为是 SVM 中的置信度（confidence） 不过，置信度始终不是概率，它没有边界，可以无限大，大部分时候也不是以百分比或者小数的形式呈现，而 SVC 的判断过程又不像决策树一样可以求解出一个比例。为了解决这个矛盾，SVC有重要参数probability 布尔值，可不填，默认 False 是否启用概率估计。进行必须在调用fit之前启用它，启用此功能会减慢 SVM 的运算速度。设置为True则会启动，启用之后，SVC的接口predict_proba和predict_log_proba将生效 。 from sklearn.datasets import make_blobs from sklearn.svm import SVC import matplotlib.pyplot as plt import numpy as np class_1 = 500 # 类别1有500个样本 class_2 = 50 # 类别2只有50个 centers = [[0.0, 0.0], [2.0, 2.0]] # 设定两个类别的中心 clusters_std = [1.5, 0.5] # 设定两个类别的方差，通常来说，样本量比较大的类别会更加松散 X, y = make_blobs(n_samples=[class_1, class_2], centers=centers, cluster_std=clusters_std, random_state=0, shuffle=False) #看看数据集长什么样 plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"rainbow\",s=10) #其中红色点是少数类，紫色点是多数类 clf_proba.predict_proba(X) # 生成的各类标签下的概率 clf_proba.predict_proba(X).shape clf_proba.decision_function(X) # 样本到决策边界的距离 clf_proba.decision_function(X).shape 在二分类过程中，decision_function只会生成一列距离，样本的类别由距离的符号来判断，但是 predict_proba会生成两个类别分别对应的概率。SVM 也可以生成概率，所以我们可以使用和逻辑回归同样的方式来在 SVM 上设定和调节我们的阈值。 在二分类过程中，有可能出现predict_proba返回的概率小于0.5，但样本依旧被标记为正类的情况出现，毕竟支持向量机本身并不依赖于概率来完成自己的分类。如果我们的确需要置信度分数，但不一定非要是概率形式的话，那建议可以将probability设置为False，使用decision_function这个接口而不是predict_proba . 重要概念 决策边界是比所在数据空间小一维的空间，在三维数据空间中就是一个平面，在二维数据空间中就是一条直线 Update time： 2020-05-23 "},"Chapter1/朴素贝叶斯.html":{"url":"Chapter1/朴素贝叶斯.html","title":"朴素贝叶斯","keywords":"","body":"朴素贝叶斯 sklearn中的朴素贝叶斯 类 含义 naive_bayes.GaussianNB 高斯分布下的朴素贝叶斯 naive_bayes.BernoulliNB 伯努利分布下的朴素贝叶斯 naive_bayes.MultinomialNB 多项式分布下的朴素贝叶斯 naive_bayes.ComplementNB 补集朴素贝叶斯 1. 高斯朴素贝叶斯 GaussianNB class sklearn.naive_bayes.GaussianNB (priors=None, var_smoothing=1e-09) 参数： prior ：可输入任何类数组结构，形状为（n_classes，） 表示类的先验概率。如果指定，则不根据数据调整先验，如果不指定，则自行根据数据计 算先验概率 $P(Y)$ var_smoothing : 浮点数，可不填（默认值= 1e-9） 在估计方差时，为了追求估计的稳定性，将所有特征的方差中最大的方差以某个比例添加 到估计的方差中。这个比例，由var_smoothing参数控制 Examples: import numpy as np import matplotlib.pyplot as plt from sklearn.naive_bayes import GaussianNB from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split digits = load_digits() #手写数据集 X, y = digits.data, digits.target # 划分数据集 Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3,random_state=420) np.unique(Ytrain) #多分类问题，类别是10个 gnb = GaussianNB().fit(Xtrain,Ytrain) #查看分数 acc_score = gnb.score(Xtest,Ytest) #返回预测的精确性 accuracy #查看预测结果 Y_pred = gnb.predict(Xtest) #查看预测的概率结果 prob = gnb.predict_proba(Xtest) prob.shape #每一列对应一个标签类别下的概率 # (540, 10) 使用混淆矩阵来查看贝叶斯的分类结果 from sklearn.metrics import confusion_matrix as CM CM(Ytest,Y_pred) #注意，ROC曲线是不能用于多分类的。多分类状况下最佳的模型评估指标是混淆矩阵和整体的准确度 #array([[47, 0, 0, 0, 0, 0, 0, 1, 0, 0], # [ 0, 46, 2, 0, 0, 0, 0, 3, 6, 2], # [ 0, 2, 35, 0, 0, 0, 1, 0, 16, 0], # [ 0, 0, 1, 40, 0, 1, 0, 3, 4, 0], # [ 0, 0, 1, 0, 39, 0, 1, 4, 0, 0], # [ 0, 0, 0, 2, 0, 58, 1, 1, 1, 0], # [ 0, 0, 1, 0, 0, 1, 49, 0, 0, 0], # [ 0, 0, 0, 0, 0, 0, 0, 54, 0, 0], # [ 0, 3, 0, 1, 0, 0, 0, 2, 55, 0], # [ 1, 1, 0, 1, 2, 0, 0, 3, 7, 41]], dtype=int64) 2. 多项式朴素贝叶斯 MultinomialNB 多项式贝叶斯可能是除了高斯之外，最为人所知的贝叶斯算法了。它也是基于原始的贝叶斯理论，但假设概率分布是服从一个简单多项式分布。多项式分布来源于统计学中的多项式实验，这种实验可以具体解释为：实验包括n次重复试验，每项试验都有不同的可能结果。在任何给定的试验中，特定结果发生的概率是不变的。 多项式分布擅长的是分类型变量 多项式实验中的实验结果都很具体，它所涉及的特征往往是次数，频率，计数，出现与否这样的概念，这些概念都是离散的正整数，因此sklearn中的多项式朴素贝叶斯不接受负值的输入 。 由于这样的特性，多项式朴素贝叶斯的特征矩阵经常是稀疏矩阵（不一定总是稀疏矩阵），并且它经常被用于文本分类 多项式模型在计算先验概率 P(yk)P(y_{k})P(y​k​​) 和条件概率 P(xi∣yk)P(x_{i}|y_{k})P(x​i​​∣y​k​​) 时，会做一些平滑处理，具体公式为： N是总的样本个数，k是总的类别个数，NykN_{y_{k}}N​y​k​​​​ 是类别为 yky_ky​k​​ 的样本个数，α\\alphaα 是平滑值。 NykN_{y_{k}}N​y​k​​​​ 是类别为 yky_ky​k​​ 的样本个数，n是特征的维数，Nyk,xiN_{y_{k},x_{i}}N​y​k​​,x​i​​​​ 是类别为 yky_ky​k​​ 的样本中，第 iii 维特征的值是xix_ix​i​​ 的样本个数，α\\alphaα 是平滑值。 在sklearn中，用来执行多项式朴素贝叶斯的类MultinomialNB包含如下的参数和属性 。 class sklearn.naive_bayes.MultinomialNB (alpha=1.0, fit_prior=True, class_prior=None) 参数： alpha : 浮点数, 可不填 (默认为1.0) 拉普拉斯或利德斯通平滑的参数 α\\alphaα ，如果设置为0则表示完全没有平滑选项。但是需要注意的是，平滑相当于人 为给概率加上一些噪音，因此 $\\alpha$ 设置得越大，多项式朴素贝叶斯的精确性会越低（虽然影响不是非常大），布里 尔分数也会逐渐升高 fit_prior : 布尔值, 可不填 (默认为True) 是否学习先验概率 。如果设置为false，则不使用先验概率，而使用统一先验概率（uniform prior），即认为每个标签类出现的概率是 1 / n_classes class_prior：形似数组的结构，结构为(n_classes, )，可不填（默认为None） 类的先验概率 P(Y=c)P(Y=c)P(Y=c)。如果没有给出具体的先验概率则自动根据数据来进行计算 Examples: 导入需要的模块和库 from sklearn.preprocessing import MinMaxScaler from sklearn.naive_bayes import MultinomialNB from sklearn.model_selection import train_test_split from sklearn.datasets import make_blobs from sklearn.metrics import brier_score_los 建立数据集 class_1 = 500 class_2 = 500 #两个类别分别设定500个样本 centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心 clusters_std = [0.5, 0.5] #设定两个类别的方差 X, y = make_blobs(n_samples=[class_1, class_2], centers=centers, cluster_std=clusters_std, random_state=0, shuffle=False ) Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y ,test_size=0.3 ,random_state=420 ) 归一化，确保输入的矩阵不带有负数 #先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数 mms = MinMaxScaler().fit(Xtrain) Xtrain_ = mms.transform(Xtrain) Xtest_ = mms.transform(Xtest) 建立一个多项式朴素贝叶斯分类器 mnb = MultinomialNB().fit(Xtrain_, Ytrain) #重要属性：调用根据数据获取的，每个标签类的对数先验概率log(P(Y)) #由于概率永远是在[0,1]之间，因此对数先验概率返回的永远是负值 mnb.class_log_prior_ # 两类的先验概率 # array([-0.69029411, -0.69600841]) mnb.class_log_prior_.shape np.unique(Ytrain) # array([0, 1]) #可以使用np.exp来查看真正的概率值 np.exp(mnb.class_log_prior_) # 两类的概率值 # array([0.50142857, 0.49857143]) #重要属性：返回一个固定标签类别下的每个特征的对数概率log(P(Xi|y)) mnb.feature_log_prob_ # array([[-0.76164788, -0.62903951], # [-0.72500918, -0.6622691 ]]) mnb.feature_log_prob_.shape # 2 个特征，2 个标签 # (2, 2) #重要属性：在fit时每个标签类别下包含的样本数。当fit接口中的sample_weight被设置时， #该接口返回的值也会受到加权的影响 mnb.class_count_ # 每个类别样本的数量 #array([351., 349.]) mnb.class_count_.shape 分类器的效果如何呢？ #一些传统的接口 mnb.predict(Xtest_) mnb.predict_proba(Xtest_) mnb.score(Xtest_,Ytest) # 0.5433333333333333 效果不太理想 #来试试看把Xtiain转换成分类型数据吧 #注意我们的Xtrain没有经过归一化，因为做哑变量之后自然所有的数据就不会又负数了 from sklearn.preprocessing import KBinsDiscretizer kbs = KBinsDiscretizer(n_bins=10, encode='onehot').fit(Xtrain) Xtrain_ = kbs.transform(Xtrain) Xtest_ = kbs.transform(Xtest) mnb = MultinomialNB().fit(Xtrain_, Ytrain) mnb.score(Xtest_,Ytest) # 0.9966666666666667 可以看出，多项式朴素贝叶斯的基本操作和代码都非常简单。同样的数据，如果采用哑变量方式的分箱处理，多项式贝叶斯的效果会突飞猛进 3. 伯努利朴素贝叶斯 BernoulliNB 多项式朴素贝叶斯可同时处理二项分布（抛硬币）和多项分布（掷骰子），其中二项分布又叫做伯努利分布，它是一种现实中常见，并且拥有很多优越数学性质的分布。因此，既然有着多项式朴素贝叶斯，我们自然也就又专门用来处理二项分布的朴素贝叶斯：伯努利朴素贝叶斯。 伯努利贝叶斯类BernoulliN假设数据服从多元伯努利分布，并在此基础上应用朴素贝叶斯的训练和分类过程。多元伯努利分布简单来说，就是数据集中可以存在多个特征，但每个特征都是二分类的，可以以布尔变量表示，也可以表示为{0，1}或者{-1，1}等任意二分类组合。因此，这个类要求将样本转换为二分类特征向量，如果数据本身不是二分类的，那可以使用类中专门用来二值化的参数binarize来改变数据。 伯努利朴素贝叶斯与多项式朴素贝叶斯非常相似，都常用于处理文本分类数据。但由于伯努利朴素贝叶斯是处理二项分布，所以它更加在意的是“存在与否”，而不是“出现多少次”这样的次数或频率，这是伯努利贝叶斯与多项式贝叶斯的根本性不同。在文本分类的情况下，伯努利朴素贝叶斯可以使用单词出现向量（而不是单词计数向量）来训练分类器。文档较短的数据集上，伯努利朴素贝叶斯的效果会更加好 class sklearn.naive_bayes.BernoulliNB (alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None) 参数： alpha : 浮点数, 可不填 (默认为1.0) 拉普拉斯或利德斯通平滑的参数 α\\alphaα ，如果设置为0则表示完全没有平滑选项。但是需要注意的是，平滑相当于人 为给概率加上一些噪音，因此 设置得越大，多项式朴素贝叶斯的精确性会越低（虽然影响不是非常大），布里 尔分数也会逐渐升高 binarize : 浮点数或None，可不填，默认为0 将特征二值化的阈值，如果设定为None，则会假定说特征已经被二值化完毕 fit_prior : 布尔值, 可不填 (默认为True) 是否学习先验概率 P(Y=c)P(Y=c)P(Y=c) 。如果设置为false，则不使用先验概率，而使用统一先验概率（uniform prior），即认为每个标签类出现的概率是 1 / n_classes. class_prior：形似数组的结构，结构为(n_classes, )，可不填（默认为None） 类的先验概率 P(Y=c)P(Y=c)P(Y=c) 。如果没有给出具体的先验概率则自动根据数据来进行计算。 Examples: from sklearn.naive_bayes import BernoulliNB #普通来说我们应该使用二值化的类sklearn.preprocessing.Binarizer来将特征一个个二值化 #然而这样效率过低，因此我们选择归一化之后直接设置一个阈值 mms = MinMaxScaler().fit(Xtrain) Xtrain_ = mms.transform(Xtrain) Xtest_ = mms.transform(Xtest) #不设置二值化 bnl_ = BernoulliNB().fit(Xtrain_, Ytrain) bnl_.score(Xtest_,Ytest) brier_score_loss(Ytest,bnl_.predict_proba(Xtest_)[:,1],pos_label=1) #设置二值化阈值为0.5 bnl = BernoulliNB(binarize=0.5).fit(Xtrain_, Ytrain) bnl.score(Xtest_,Ytest) brier_score_loss(Ytest,bnl.predict_proba(Xtest_)[:,1],pos_label=1) 4. 补集朴素贝叶斯ComplementNB 补集朴素贝叶斯（complement naive Bayes，CNB）算法是标准多项式朴素贝叶斯算法的改进。CNB的发明小组创造出CNB的初衷是为了解决贝叶斯中的“朴素”假设带来的各种问题，他们希望能够创造出数学方法以逃避朴素贝叶斯中的朴素假设，让算法能够不去关心所有特征之间是否是条件独立的。以此为基础，他们创造出了能够解决样本不平衡问题，并且能够一定程度上忽略朴素假设的补集朴素贝叶斯。 在实验中，CNB的参数估计已经被证明比普通多项式朴素贝叶斯更稳定，并且它特别适合于样本不平衡的数据集。有时候，CNB在文本分类任务上的表现有时能够优于多项式朴素贝叶斯，因此现在补集朴素贝叶斯也开始逐渐流行 在sklearn中，补集朴素贝叶斯由类ComplementNB完成，它包含的参数和多项式贝叶斯也非常相似： class sklearn.naive_bayes.ComplementNB (alpha=1.0, fit_prior=True, class_prior=None, norm=False) 参数： alpha : 浮点数, 可不填 (默认为1.0) 拉普拉斯或利德斯通平滑的参数 α\\alphaα ，如果设置为0则表示完全没有平滑选项。但是需要注意的是，平滑相当于人 为给概率加上一些噪音，因此 $\\alpha$ 设置得越大，多项式朴素贝叶斯的精确性会越低（虽然影响不是非常大），布里 尔分数也会逐渐升高 norm : 布尔值，可不填，默认False 在计算权重的时候是否适用L2范式来规范权重的大小。默认不进行规范，即不跟从补集朴素贝叶斯算法的全部 内容，如果希望进行规范，请设置为True fit_prior : 布尔值, 可不填 (默认为True) 是否学习先验概率 。如果设置为false，则不使用先验概率，而使用统一先验概率（uniform prior），即认为每个标签类出现的概率是 1 / n_classes class_prior：形似数组的结构，结构为(n_classes, )，可不填（默认为None） 类的先验概率 P(Y=c)P(Y=c)P(Y=c)。如果没有给出具体的先验概率则自动根据数据来进行计算 Examples: 那来看看，补集朴素贝叶斯在不平衡样本上的表现吧，同时我们来计算一下每种贝叶斯的计算速度： from sklearn.naive_bayes import ComplementNB from time import time import datetime name = [\"Multinomial\",\"Gaussian\",\"Bernoulli\",\"Complement\"] models = [MultinomialNB(),GaussianNB(),BernoulliNB(),ComplementNB()] for name,clf in zip(name,models): times = time() Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y ,test_size=0.3 ,random_state=420) #预处理 if name != \"Gaussian\": kbs = KBinsDiscretizer(n_bins=10, encode='onehot').fit(Xtrain) Xtrain = kbs.transform(Xtrain) Xtest = kbs.transform(Xtest) clf.fit(Xtrain,Ytrain) y_pred = clf.predict(Xtest) proba = clf.predict_proba(Xtest)[:,1] score = clf.score(Xtest,Ytest) print(name) print(\"\\tBrier:{:.3f}\".format(BS(Ytest,proba,pos_label=1))) print(\"\\tAccuracy:{:.3f}\".format(score)) print(\"\\tRecall:{:.3f}\".format(recall_score(Ytest,y_pred))) print(\"\\tAUC:{:.3f}\".format(AUC(Ytest,proba))) print(datetime.datetime.fromtimestamp(time()-times).strftime(\"%M:%S:%f\")) 输出： Multinomial Brier:0.007 Accuracy:0.990 Recall:0.000 AUC:0.991 00:00:050863 Gaussian Brier:0.006 Accuracy:0.990 Recall:0.438 AUC:0.993 00:00:038898 Bernoulli Brier:0.009 Accuracy:0.987 Recall:0.771 AUC:0.987 00:00:049866 Complement Brier:0.038 Accuracy:0.953 Recall:0.987 AUC:0.991 00:00:044881 参考 朴素贝叶斯的三个常用模型：高斯、多项式、伯努利 朴素贝叶斯理论推导与三种常见模型 Update time： 2020-05-23 "},"Chapter1/回归大家族.html":{"url":"Chapter1/回归大家族.html","title":"回归大家族","keywords":"","body":"线性回归 sklearn中的线性模型模块是linear_model，我们曾经在学习逻辑回归的时候提到过这个模块。linear_model包含了多种多样的类和函数，其中逻辑回归相关的类和函数在这里就不给大家列举了。今天的课中我将会为大家来讲解：普通线性回归，多项式回归，岭回归，LASSO，以及弹性网 。 类/函数 含义 linear_model.LinearRegression 使用普通最小二乘法的线性回归 岭回归 linear_model.Ridge 岭回归，一种将 L2 作为正则化工具的线性最小二乘回归 LASSO linear_model.Lasso Lasso，使用L1作为正则化工具来训练的线性回归模型 多元线性回归LinearRegression class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None ) 参数 参数 含义 fit_intercept 布尔值，可不填，默认为True 是否计算此模型的截距。如果设置为False，则不会计算截距 normalize 布尔值，可不填，默认为False 当fit_intercept设置为False时，将忽略此参数。如果为True，则特征矩阵X在进入回归之前将 会被减去均值（中心化）并除以L2范式（缩放）。如果你希望进行标准化，请在fit数据之前 使用preprocessing模块中的标准化专用类StandardScaler copy_X 布尔值，可不填，默认为True 如果为真，将在X.copy()上进行操作，否则的话原本的特征矩阵X可能被线性回归影响并覆盖 n_jobs 整数或者None，可不填，默认为None 用于计算的作业数。只在多标签的回归和数据量足够大的时候才生效。除非None在 joblib.parallel_backend上下文中，否则None统一表示为1。如果输入 -1，则表示使用全部 的CPU来进行计算。 属性 属性 含义 coef_ 数组，形状为 (n_features, )或者(n_targets, n_features) 线性回归方程中估计出的系数。如果在fit中传递多个标签（当y为二维或以上的时候），则返回 的系数是形状为（n_targets，n_features）的二维数组，而如果仅传递一个标签，则返回的系 数是长度为n_features的一维数组 intercept_ 数组，线性回归中的截距项。 linear_model.Ridge class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None ) 和线性回归相比，岭回归的参数稍微多了那么一点点，但是真正核心的参数就是我们的正则项的系数 α\\alphaα ，其他的参数是当我们希望使用最小二乘法之外的求解方法求解岭回归的时候才需要的，通常我们完全不会去触碰这些参数。所以大家只需要了解 α\\alphaα 的用法就可以了 非线性问题：多项式回归 对于回归问题，数据若能分布为一条直线，则是线性的，否则是非线性。对于分类问题，数据分布若能使用一条直线来划分类别，则是线性可分的，否则数据则是线性不可分的 。 线性模型与非线性模型 在回归中，线性数据可以使用如下的方程来进行拟合 ： y=w0+w1x1+w2x2+w3x3…wnxn y=w_{0}+w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{3} \\ldots w_{n} x_{n} y=w​0​​+w​1​​x​1​​+w​2​​x​2​​+w​3​​x​3​​…w​n​​x​n​​ 也就是我们的线性回归的方程。根据线性回归的方程，我们可以拟合出一组参数 ，在这一组固定的参数下我们可以建立一个模型，而这个模型就被我们称之为是线性回归模型。所以建模的过程就是寻找参数的过程。此时此刻我们建立的线性回归模型，是一个用于拟合线性数据的线性模型。作为线性模型的典型代表，我们可以从线性回归的方程中总结出线性模型的特点：其自变量都是一次项。 多项式回归PolynomialFeatures class sklearn.preprocessing.PolynomialFeatures (degree=2, interaction_only=False, include_bias=True ) 参数 含义 degree 多项式中的次数，默认为2 interaction_only 布尔值是否只产生交互项，默认为False include_bias 布尔值，是否产出与截距项相乘的 ，默认True from sklearn.preprocessing import PolynomialFeatures import numpy as np #如果原始数据是一维的 X = np.arange(1,4).reshape(-1,1) X # 二次多项式，参数degree控制多项式的次方 poly = PolynomialFeatures(degree=2) #接口transform直接调用 X_ = poly.fit_transform(X) X_ X_.shape #三次多项式 PolynomialFeatures(degree=3).fit_transform(X) 多项式变化后数据看起来不太一样了：首先，数据的特征（维度）增加了，这正符合我们希望的将数据转换到高维空间的愿望。其次，维度的增加是有一定的规律的。不难发现，如果我们本来的特征矩阵中只有一个特征 xxx，而转换后我们得到： 这个规律在转换为二次多项式的时候同样适用。原本，我们的模型应该是形似 的结构，而转换后我们的特征变化导致了模型的变化。根据我们在支持向量机中的经验，现在这个被投影到更高维空间中的数据在某个角度上看起来已经是一条直线了，于是我们可以继续使用线性回归来进行拟合。线性回归是会对每个特征拟合出权重 的，所以当我们拟合高维数据的时候，我们会得到下面的模型 y=w0x0+w1x+w2x2+w3x3,(x0=1) y=w_{0} x_{0}+w_{1} x+w_{2} x^{2}+w_{3} x^{3}, \\quad\\left(x_{0}=1\\right) y=w​0​​x​0​​+w​1​​x+w​2​​x​2​​+w​3​​x​3​​,(x​0​​=1) 由此推断，假设多项式转化的次数是n，则数据会被转化成形如： [1,x,x2,x3…xn] \\left[1, x, x^{2}, x^{3} \\ldots x^{n}\\right] [1,x,x​2​​,x​3​​…x​n​​] 而拟合出的方程也可以被改写成： y=w0x0+w1x+w2x2+w3x3…wnxn,(x0=1) y=w_{0} x_{0}+w_{1} x+w_{2} x^{2}+w_{3} x^{3} \\ldots w_{n} x^{n},\\left(x_{0}=1\\right) y=w​0​​x​0​​+w​1​​x+w​2​​x​2​​+w​3​​x​3​​…w​n​​x​n​​,(x​0​​=1) 这个过程看起来非常简单，只不过是将原始的 xxx 上的次方增加，并且为这些次方项都加上权重 www ，然后增加一列所有次方为0的列作为截距乘数的 x0x_0x​0​​ ，参数include_bias就是用来控制 的生成的 x0x_0x​0​​ . #三次多项式，不带与截距项相乘的x0 PolynomialFeatures(degree=3,include_bias=False).fit_transform(X) #为什么我们会希望不生成与截距相乘的x0呢？ #对于多项式回归来说，我们已经为线性回归准备好了x0，但是线性回归并不知道 xxx = PolynomialFeatures(degree=3).fit_transform(X) xxx.shape rnd = np.random.RandomState(42) #设置随机数种子 y = rnd.randn(3) y # 生成了多少个系数？ LinearRegression().fit(xxx,y).coef_ #查看截距 LinearRegression().fit(xxx,y).intercept_ #发现问题了吗？线性回归并没有把多项式生成的x0当作是截距项 #所以我们可以选择：关闭多项式回归中的include_bias #也可以选择：关闭线性回归中的fit_intercept #生成了多少个系数？ LinearRegression(fit_intercpet=False).fit(xxx,y).coef_ #查看截距 LinearRegression(fit_intercpet=False).fit(xxx,y).intercept_ 不过，这只是一维状况的表达，大多数时候我们的原始特征矩阵不可能会是一维的，至少也是二维以上，很多时候还可能存在上千个特征或者维度。现在我们来看看原始特征矩阵是二维的状况： X = np.arange(6).reshape(3, 2) X # 尝试二次多项式 PolynomialFeatures(degree=2).fit_transform(X) 很明显，上面一维的转换公式已经不适用了，但如果我们仔细看，是可以看出这样的规律的 : 当原始特征为二维的时候，多项式的二次变化突然将特征增加到了六维，其中一维是常量（也就是截距）。当我们继续适用线性回归去拟合的时候，我们会得到的方程如下 : 这个时候大家可能就会感觉到比较困惑了，怎么会出现这样的变化？如果想要总结这个规律，可以继续来尝试三次多项式 # 尝试三次多项式 PolynomialFeatures(degree=3).fit_transform(X) 不难发现：当我们进行多项式转换的时候，多项式会产出到最高次数为止的所有低高次项 比如如果我们规定多项式的次数为2，多项式就会产出所有次数为1和次数为2的项反馈给我们，相应的如果我们规定多项式的次数为n，则多式会产出所有从次数为1到次数为n的项。注意 x1x2x_1x_2x​1​​x​2​​ 和x12x_{1}^{2}x​1​2​​ 一样都是二次项，一个自变量的平方其实也就相当于是 x1x2x_1x_2x​1​​x​2​​ ，所以在三次多项式中 x12x2x_1^{2}x_2x​1​2​​x​2​​ 就是三次项。 在多项式回归中，我们可以规定是否产生平方或者立方项，其实如果我们只要求高次项的话， 会是一个比 更好的高次项，因为 x1x2x_1x_2x​1​​x​2​​ 和x1x_1x​1​​之间的共线性会比x12x_1^{2}x​1​2​​与x1x_1x​1​​之间的共线性好那么一点点（只是一点点），而我们多项式转化之后是需要使用线性回归模型来进行拟合的，就算机器学习中不是那么在意数据上的基本假设，但是太过分的共线性还是会影响到模型的拟合。 因此sklearn中存在着控制是否要生成平方和立方项的参数interaction_only，默认为 False，以减少共线性 。 PolynomialFeatures(degree=2).fit_transform(X) PolynomialFeatures(degree=2,interaction_only=True).fit_transform(X) #对比之下，当interaction_only为True的时候，只生成交互项 随着多项式的次数逐渐变高，特征矩阵会被转化得越来越复杂。不仅是次数，当特征矩阵中的维度数（特征数）增加的时候，多项式同样会变得更加复杂 ： #更高维度的原始特征矩阵 X = np.arange(9).reshape(3, 3) X PolynomialFeatures(degree=2).fit_transform(X) PolynomialFeatures(degree=3).fit_transform(X) X_ = PolynomialFeatures(degree=20).fit_transform(X) X_.shape 多项式变化对于数据会有怎样的影响：随着原特征矩阵的维度上升，随着我们规定的最高次数的上升，数据会变得越来越复杂，维度越来越多，并且这种维度的增加并不能用太简单的数学公式表达出来。因此，多项式回归没有固定的模型表达式，多项式回归的模型最终长什么样子是由数据和最高次数决定的，因此我们无法断言说某个数学表达式\"就是多项式回归的数学表达\"，因此要求解多项式回归不是一件容易的事儿 。 Update time： 2020-05-23 "},"Chapter2/":{"url":"Chapter2/","title":"sklearn.metrics","keywords":"","body":"sklearn.metrics Update time： 2020-05-23 "},"Chapter2/sklearn.metrics.mean_squared_error用法.html":{"url":"Chapter2/sklearn.metrics.mean_squared_error用法.html","title":"sklearn.metrics.mean_squared_error用法","keywords":"","body":"均方误差 metrics.mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’) 参数： y_true：真实值。 y_pred：预测值。 sample_weight：样本权值。 multioutput：多维输入输出，默认为’uniform_average’，计算所有元素的均方误差，返回为一个标量；也可选‘raw_values’，计算对应列的均方误差，返回一个与列数相等的一维数组。 from sklearn.metrics import mean_squared_error y_true = [3, -1, 2, 7] y_pred = [2, 0.0, 2, 8] mean_squared_error(y_true, y_pred) # 结果为：0.75 y_true = [[0.5, 1],[-1, 1],[7, -6]] y_pred = [[0, 2],[-1, 2],[8, -5]] mean_squared_error(y_true, y_pred) # 结果为：0.7083333333333334 mean_squared_error(y_true, y_pred, multioutput='raw_values') # 结果为：array([0.41666667, 1. ]) mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7]) # 结果为：0.825 # multioutput=[0.3, 0.7]返回将array([0.41666667, 1. ])按照0.3*0.41666667+0.7*1.0计算所得的结果 mean_squared_error(y_true, y_pred, multioutput='uniform_average') # 结果为：0.7083333333333334 RMSE(均方根误差) 定义 即 均方误差开平方 RMSE，全称是Root Mean Square Error，即均方根误差，它表示预测值和观测值之间差异（称为残差）的样本标准差。均方根误差为了说明样本的离散程度。做非线性拟合时, RMSE越小越好。 标准差与均方根误差的区别 标准差是用来衡量一组数自身的离散程度，而均方根误差是用来衡量观测值同真值之间的偏差，它们的研究对象和研究目的不同，但是计算过程类似。 均方根误差算的是观测值与其真值，或者观测值与其模拟值之间的偏差，而不是观测值与其平均值之间的偏差。 Update time： 2020-08-04 "},"Chapter2/sklearn.metrics.mean_absolute_error用法.html":{"url":"Chapter2/sklearn.metrics.mean_absolute_error用法.html","title":"sklearn.metrics.mean_absolute_error用法","keywords":"","body":"sklearn.metrics.mean_absolute_error用法 平均绝对误差(MAE) Mean Absolute Error ，平均绝对误差 它表示预测值和观测值之间绝对误差的平均值。 是绝对误差的平均值 能更好地反映预测值误差的实际情况. MAE(X,h)=1m∑i=1m∣h(xi)−yi∣ M A E(X, h)=\\frac{1}{m} \\sum_{i=1}^{m}\\left|h\\left(x_{i}\\right)-y_{i}\\right| MAE(X,h)=​m​​1​​​i=1​∑​m​​∣h(x​i​​)−y​i​​∣ sklearn.metrics.mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average') 参数 y_true ：array-like of shape (n_samples,) or (n_samples, n_outputs) Ground truth (correct) target values. y_pred ： array-like of shape (n_samples,) or (n_samples, n_outputs) Estimated target values. sample_weight ：array-like of shape (n_samples,), optional 样本权重 multioutput ：string in [‘raw_values’, ‘uniform_average’] or array-like of shape (n_outputs) 定义多个输出值的聚合。类似数组的值定义用于平均错误的权重。 ‘raw_values’ : Returns a full set of errors in case of multioutput input. ‘uniform_average’ : Errors of all outputs are averaged with uniform weight. 返回值： loss：float or ndarray of floats If multioutput is ‘raw_values’, then mean absolute error is returned for each output separately. If multioutput is ‘uniform_average’ or an ndarray of weights, then the weighted average of all output errors is returned. MAE output is non-negative floating point. The best value is 0.0. Examples from sklearn.metrics import mean_absolute_error y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] mean_absolute_error(y_true, y_pred) # 0.5 y_true = [[0.5, 1], [-1, 1], [7, -6]] y_pred = [[0, 2], [-1, 2], [8, -5]] mean_absolute_error(y_true, y_pred) # 0.75 mean_absolute_error(y_true, y_pred, multioutput='uniform_average') # 0.75 mean_absolute_error(y_true, y_pred, multioutput='raw_values') # array([0.5, 1. ]) mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7]) # 0.85 # 0.5*0.3+1*0.7=0.85 Update time： 2020-08-04 "},"Chapter2/sklearn.metrics.r2_score用法.html":{"url":"Chapter2/sklearn.metrics.r2_score用法.html","title":"sklearn.metrics.r2_score用法","keywords":"","body":"$R^2$ 计算 是否拟合了足够的信息 对于回归类算法而言，只探索数据预测是否准确是不足够的。除了数据本身的数值大小之外，我们还希望我们的模型能够捕捉到数据的”规律“，比如数据的分布规律，单调性等等，而是否捕获了这些信息并无法使用MSE来衡量 。 来看这张图，其中红色线是我们的真实标签，而蓝色线是我们的拟合模型。这是一种比较极端，但的确可能发生的情况。这张图像上，前半部分的拟合非常成功，看上去我们的真实标签和我们的预测结果几乎重合，但后半部分的拟合却非常糟糕，模型向着与真实标签完全相反的方向去了。对于这样的一个拟合模型，如果我们使用MSE来对它进行判断，它的MSE会很小，因为大部分样本其实都被完美拟合了，少数样本的真实值和预测值的巨大差异在被均分到每个样本上之后，MSE就会很小。但这样的拟合结果必然不是一个好结果，因为一旦我的新样本是处于拟合曲线的后半段的，我的预测结果必然会有巨大的偏差，而这不是我们希望看到的。所以，我们希望找到新的指标，除了判断预测的数值是否正确之外，还能够判断我们的模型是否拟合了足够多的，数值之外的信息 降维算法PCA，使用方差来衡量数据上的信息量。如果方差越大，代表数据上的信息量越多，而这个信息量不仅包括了数值的大小，还包括了我们希望模型捕捉的那些规律。为了衡量模型对数据上的信息量的捕捉，我们定义了 R2R^2R​2​​ 来帮助我们： 其中 yyy 是我们的真实标签，y^\\hat y​y​^​​ 是我们的预测结果, y¯\\bar y​y​¯​​ 是我们的均值，y−y¯y - \\bar yy−​y​¯​​ 如果除以样本量m就是我们的方差。方差的本质是任意一个 yyy 值和样本均值的差异，差异越大，这些值所带的信息越多。在 R2R^2R​2​​ 中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量 , 分母是真实标签所带的信息量，所以其衡量的是1−1 -1−我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例 , 所以， R2R^2R​2​​ 越接近 1 越好。 R2R^2R​2​​可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入\"r2\"来调用 metrics.r2_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average' ) from sklearn.metrics import r2_score y_true = [1,2,4] y_pred = [1.3,2.5,3.7] r2_score(y_true,y_pred) Update time： 2020-08-04 "},"Chapter2/sklearn.metrics.explained_variance_score用法.html":{"url":"Chapter2/sklearn.metrics.explained_variance_score用法.html","title":"sklearn.metrics.explained_variance_score用法","keywords":"","body":"sklearn.metrics.explained_variance_score用法 解释回归模型的方差得分，其值取值范围是[0,1]，越接近于1说明自变量越能解释因变量 的方差变化，值越小则说明效果越差。 解释方差的得分，计算公式为： sklearn.metrics.explained_variance_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average') Parameters : y_true: array-like of shape (n_samples,) or (n_samples, n_outputs) Ground truth (correct) target values. y_pred: array-like of shape (n_samples,) or (n_samples, n_outputs) Estimated target values. sample_weight :array-like of shape (n_samples,), optional Sample weights. multioutput ：string in [‘raw_values’, ‘uniform_average’, ‘variance_weighted’] or array-like of shape (n_outputs) Defines aggregating of multiple output scores. Array-like value defines weights used to average scores. ‘raw_values’ : Returns a full set of scores in case of multioutput input. ‘uniform_average’ : Scores of all outputs are averaged with uniform weight. ‘variance_weighted’ :| Scores of all outputs are averaged, weighted by the variances of each individual output Returns： score :float or ndarray of floats The explained variance or ndarray if ‘multioutput’ is ‘raw_values’. Examples: y_true = [3, -0.5, 2, 7] y_pred = [2.5, 0.0, 2, 8] explained_variance_score(y_true, y_pred) # 0.9571734475374732 y_true = [[0.5, 1], [-1, 1], [7, -6]] y_pred = [[0, 2], [-1, 2], [8, -5]] explained_variance_score(y_true, y_pred, multioutput='uniform_average') # 0.9838709677419355 print(explained_variance_score(y_test,y_pred)) print(1-np.var(y_test-y_pred)/np.var(y_test)) Update time： 2020-08-04 "},"Chapter2/标准差计算.html":{"url":"Chapter2/标准差计算.html","title":"标准差计算","keywords":"","body":"标准差计算 标准差是用来衡量一组数自身的离散程度; 标准差是方差的平方根： numpy.std() 求标准差的时候默认是除以 n 的，即是有偏的，np.std无偏样本标准差方式为加入参数 ddof = 1； pandas.std() 默认是除以n-1 的，即是无偏的，如果想和numpy.std() 一样有偏，需要加上参数ddof=0 ，即pandas.std(ddof=0) ；DataFrame的describe()中就包含有std()； >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.std(a, ddof = 1) 3.0276503540974917 >>> np.sqrt(((a - np.mean(a)) ** 2).sum() / (a.size - 1)) 3.0276503540974917 >>> np.sqrt(( a.var() * a.size) / (a.size - 1)) 3.0276503540974917 参考： python 标准差计算（std） Update time： 2020-08-04 "},"Chapter2/sklearn.metrics.confusion_matrix用法.html":{"url":"Chapter2/sklearn.metrics.confusion_matrix用法.html","title":"sklearn.metrics.confusion_matrix用法","keywords":"","body":"sklearn.metrics.confusion_matrix用法 混淆矩阵 sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None ) 参数： y_true: array, shape = [n_samples] 是样本真实分类结果， y_pred: array, shape = [n_samples] 是样本预测分类结果 labels：array, shape = [n_classes], optional 是所给出的类别，通过这个可对类别进行选择, 少数类在前，多数类在后 sample_weight: 样本权重 返回值： C : array, shape = [n_classes, n_classes] Confusion matrix coding >>> from sklearn.metrics import confusion_matrix # 三分类的情况 >>> y_true = [2, 1, 0, 1, 2, 0] >>> y_pred = [2, 0, 0, 1, 2, 1] >>> confusion_matrix(y_true, y_pred) array([[1, 1, 0], [1, 1, 0], [0, 0, 2]], dtype=int64) 自建数据集 class_1_ = 7 class_2_ = 4 centers_ = [[0.0, 0.0], [1,1]] clusters_std = [0.5, 1] X_, y_ = make_blobs(n_samples=[class_1_, class_2_], centers=centers_, cluster_std=clusters_std, random_state=0, shuffle=False) plt.scatter(X_[:, 0], X_[:, 1], c=y_, cmap=\"rainbow\",s=30) 建模，调用概率 from sklearn.linear_model import LogisticRegression as LogiR clf_lo = LogiR().fit(X_,y_) prob = clf_lo.predict_proba(X_) #将样本和概率放到一个DataFrame中 import pandas as pd prob = pd.DataFrame(prob) prob.columns = [\"0\",\"1\"] 使用阈值0.5，大于0.5的样本被预测为1，小于0.5的样本被预测为0 #手动调节阈值，来改变我们的模型效果 for i in range(prob.shape[0]): if prob.loc[i,\"1\"] > 0.5: # 添加新的标签列 prob.loc[i,\"pred\"] = 1 else: # 添加新的标签列 prob.loc[i,\"pred\"] = 0 # 添加真实的标签列 prob[\"y_true\"] = y_ prob = prob.sort_values(by=\"1\",ascending=False) 使用混淆矩阵查看结果 from sklearn.metrics import confusion_matrix as CM CM(prob.loc[:,\"y_true\"],prob.loc[:,\"pred\"],labels=[1,0]) #array([[2, 2], # [1, 6]], dtype=int64) Update time： 2020-05-23 "},"Chapter2/sklearn.metrics.accuracy_score用法.html":{"url":"Chapter2/sklearn.metrics.accuracy_score用法.html","title":"sklearn.metrics.accuracy_score用法","keywords":"","body":"sklearn.metrics.accuracy_score用法 准确率accuracy :所有的测量点到真实值非常接近。与测量点的偏差有关。 sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None ) Parameters： y_true ：1d array-like, or label indicator array / sparse matrix y_pred ：1d array-like, or label indicator array / sparse matrix normalize：bool, optional (default=True) 如果为False，则返回正确分类的样本数。 否则，返回正确分类的样本的分数。 sample_weight： array-like of shape (n_samples,), default=None Returns： score：float coding >>> from sklearn.metrics import accuracy_score >>> y_pred = [0, 2, 1, 3] >>> y_true = [0, 1, 2, 3] >>> accuracy_score(y_true, y_pred) 0.5 >>> accuracy_score(y_true, y_pred, normalize=False) 2 Update time： 2020-05-23 "},"Chapter2/sklearn.metrics.precision_score用法.html":{"url":"Chapter2/sklearn.metrics.precision_score用法.html","title":"sklearn.metrics.precision_score用法","keywords":"","body":"sklearn.metrics.precision_score用法 精确度 precision :所有的测量点到测量点集合的均值非常接近，与测量点的方差有关。就是说各个点紧密的聚合在一起。 sklearn.metrics.precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn' ) Parameters: y_true :1d array-like, or label indicator array / sparse matrix y_pred :1d array-like, or label indicator array / sparse matrix average : 计算类型 string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’] average参数定义了该指标的计算方法，二分类时average参数默认是binary，多分类时，可选参数有micro、macro、weighted和samples。 sample_weight : 样本权重 参数average 选项 含义 binary 二分类 micro 统计全局TP和FP来计算 macro 计算每个标签的未加权均值（不考虑不平衡） weighted 计算每个标签等等加权均值（考虑不平衡） samples 计算每个实例找出其均值 None 返回每类的精确度 Returns: precision:float (if average is not None) or array of float, shape = [n_unique_labels] >>> from sklearn.metrics import precision_score >>> y_true = [0, 1, 2, 0, 1, 2] >>> y_pred = [0, 2, 1, 0, 0, 1] >>> precision_score(y_true, y_pred, average='macro') 0.22... >>> precision_score(y_true, y_pred, average='micro') 0.33... >>> precision_score(y_true, y_pred, average='weighted') 0.22... >>> precision_score(y_true, y_pred, average=None) array([0.66..., 0. , 0. ]) >>> y_pred = [0, 0, 0, 0, 0, 0] >>> precision_score(y_true, y_pred, average=None) array([0.33..., 0. , 0. ]) >>> precision_score(y_true, y_pred, average=None, zero_division=1) array([0.33..., 1. , 1. ]) micro、macro、weighted以及样本不均时加入sample_weight参数的计算方法。 以三分类模型举例。首先我们生成一组数据： import numpy as np y_true = np.array([-1]*30 + [0]*240 + [1]*30) y_pred = np.array([-1]*10 + [0]*10 + [1]*10 + [-1]*40 + [0]*160 + [1]*40 + [-1]*5 + [0]*5 + [1]*20) 数据分为-1、0、1三类，真实数据y_true中，一共有30个-1，240个0，30个1。然后我们生成真实数据y_true和预测数据y_pred的混淆矩阵，之后的演示中我们会用到混淆矩阵的数据： confusion_matrix(y_true, y_pred) #array([[ 10, 10, 10], # [ 40, 160, 40], # [ 5, 5, 20]], dtype=int64) 由混淆矩阵我们可以计算出真正类数TP、假正类数FP、假负类数FN，如下： TP FN FP -1 10 20 45 0 160 80 15 1 20 10 50 以precision_score的计算为例，accuracy_score、recall_score、f1_score等均可以此类推。 sklearn包中计算precision_score klearn.metrics.precision_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sample_weight=None) 其中，average参数定义了该指标的计算方法，二分类时average参数默认是binary，多分类时，可选参数有micro、macro、weighted和samples。samples的用法我也不是很明确，所以本文只讲解micro、macro、weighted。 1 不加sample_weight 1.1 micro micro算法是指把所有的类放在一起算，具体到precision，就是把所有类的 TP 加和，再除以所有类的 TP 和 FN 的加和。因此micro方法下的precision和recall都等于accuracy。 1.2 macro macro方法就是先分别求出每个类的precision再算术平均。 1.3 weighted 前面提到的macro算法是取算术平均，weighted算法就是在macro算法的改良版，不再是取算术平均、乘以固定weight（也就是1/3）了，而是乘以该类在总样本数中的占比。计算一下每个类的占比： >>> w_neg1, w_0, w_pos1 = np.bincount(y_true+1) / len(y_true) >>> print(w_neg1, w_0, w_pos1) 0.1 0.8 0.1 然后手算一下weighted方法下的precision： 2 加入sample weight 当样本不均衡时，比如本文举出的样本，中间的0占80%，1和-1各占10%，每个类数量差距很大，我们可以选择加入sample_weight来调整我们的样本。 首先我们使用sklearn里的compute_sample_weight函数来计算sample_weight： sw = compute_sample_weight(class_weight='balanced',y=y_true) sw 是一个和 y_true 的 shape 相同的数据，每一个数代表该样本所在的 sample_weight。它的具体计算方法是 : 总样本数 /（类数 * 每个类的个数），比如一个值为-1的样本，它的sample_weight就是300 / (3 * 30)。 使用sample_weight计算出的混淆矩阵如下： >>> cm =confusion_matrix(y_true, y_pred, sample_weight=sw) >>> cm array([[33.33333333, 33.33333333, 33.33333333], [16.66666667, 66.66666667, 16.66666667], [16.66666667, 16.66666667, 66.66666667]]) 由该混淆矩阵可以得到TP、FN、FP: TP FN FP -1 33.3 66.67 33.33 0 66.67 33.33 50 1 66.67 33.33 50 三种precision的计算方法和第一节中计算的一样，就不多介绍了。使用sklearn的函数时，把sw作为函数的sample_weight参数输入即可。 参考 详解sklearn的多分类模型评价指标 Update time： 2020-05-23 "},"Chapter2/sklearn.metrics.recall_score用法.html":{"url":"Chapter2/sklearn.metrics.recall_score用法.html","title":"sklearn.metrics.recall_score用法","keywords":"","body":"sklearn.metrics.recall_score用法 召回率recall sklearn.metrics.precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn' ) Parameters: y_true :1d array-like, or label indicator array / sparse matrix y_pred :1d array-like, or label indicator array / sparse matrix average : 计算类型 string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’] average参数定义了该指标的计算方法，二分类时average参数默认是binary，多分类时，可选参数有micro、macro、weighted和samples。 sample_weight : 样本权重 参数average 选项 含义 binary 二分类 micro 统计全局TP和FP来计算 macro 计算每个标签的未加权均值（不考虑不平衡） weighted 计算每个标签等等加权均值（考虑不平衡） samples 计算每个实例找出其均值 None 返回每类的精确度 Returns: precision:float (if average is not None) or array of float, shape = [n_unique_labels] Examples: >>> from sklearn.metrics import recall_score >>> y_true = [0, 1, 2, 0, 1, 2] >>> y_pred = [0, 2, 1, 0, 0, 1] >>> recall_score(y_true, y_pred, average='macro') 0.33... >>> recall_score(y_true, y_pred, average='micro') 0.33... >>> recall_score(y_true, y_pred, average='weighted') 0.33... >>> recall_score(y_true, y_pred, average=None) array([1., 0., 0.]) >>> y_true = [0, 0, 0, 0, 0, 0] >>> recall_score(y_true, y_pred, average=None) array([0.5, 0. , 0. ]) >>> recall_score(y_true, y_pred, average=None, zero_division=1) array([0.5, 1. , 1. ]) Update time： 2020-05-23 "},"Chapter2/sklearn.metrics.f1_score用法.html":{"url":"Chapter2/sklearn.metrics.f1_score用法.html","title":"sklearn.metrics.f1_score用法","keywords":"","body":"sklearn.metrics.f1_score用法 F1 measure sklearn.metrics.precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn' ) Parameters: y_true :1d array-like, or label indicator array / sparse matrix y_pred :1d array-like, or label indicator array / sparse matrix average : 计算类型 string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’] average参数定义了该指标的计算方法，二分类时average参数默认是binary，多分类时，可选参数有micro、macro、weighted和samples。 sample_weight : 样本权重 参数average 选项 含义 binary 二分类 micro 统计全局TP和FP来计算 macro 计算每个标签的未加权均值（不考虑不平衡） weighted 计算每个标签等等加权均值（考虑不平衡） samples 计算每个实例找出其均值 None 返回每类的精确度 Returns: precision:float (if average is not None) or array of float, shape = [n_unique_labels] >>> from sklearn.metrics import f1_score >>> y_true = [0, 1, 2, 0, 1, 2] >>> y_pred = [0, 2, 1, 0, 0, 1] >>> f1_score(y_true, y_pred, average='macro') 0.26... >>> f1_score(y_true, y_pred, average='micro') 0.33... >>> f1_score(y_true, y_pred, average='weighted') 0.26... >>> f1_score(y_true, y_pred, average=None) array([0.8, 0. , 0. ]) >>> y_true = [0, 0, 0, 0, 0, 0] >>> y_pred = [0, 0, 0, 0, 0, 0] >>> f1_score(y_true, y_pred, zero_division=1) 1.0... Update time： 2020-05-23 "},"Chapter2/sklearn.metrics.roc_curve用法.html":{"url":"Chapter2/sklearn.metrics.roc_curve用法.html","title":"sklearn.metrics.roc_curve用法","keywords":"","body":"sklearn.metrics.roc_curve用法 ROC 是一条以不同阈值下的假正率 FPR 为横坐标，不同阈值下的召回率 Recall 为纵坐标的曲线。 建立 ROC 曲线的根本目的是找寻 Recall 和 FPR 之间的平衡，让我们能够衡量模型在尽量捕捉少数类的时候，误伤多数类的情况会如何变化。横坐标是FPR，代表着模型将多数类判断错误的能力，纵坐标Recall，代表着模型捕捉少数类的能力，所以ROC曲线代表着，随着Recall的不断增加，FPR如何增加。我们希望随着Recall的不断提升，FPR增加得越慢越好，这说明我们可以尽量高效地捕捉出少数类，而不会将很多地多数类判断错误 sklearn.metrics.roc_curve ( y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True ) 参数： y_true : 数组，形状 = [n_samples]，真实标签 y_score : 数组，形状 = [n_samples]，置信度分数，可以是正类样本的概率值，或置信度分数，或者SVC 中decision_function返回的距离 pos_label : 整数或者字符串, 默认None，表示被认为是正类样本的类别 . 即标签中认定为正的label个数。 例如label= [1,2,3,4]，如果设置pos_label = 2,则认为3,4为positive，其他均为negtive。 sample_weight : 形如 [n_samples]的类数组结构，可不填，表示样本的权重 drop_intermediate : 布尔值，默认True，如果设置为True，表示会舍弃一些ROC曲线上不显示的阈值点，这对于计算一个比较轻量的ROC曲线来说非常有用 返回值： FPR，Recall以及阈值。 Example 1 from sklearn.datasets import make_blobs from sklearn.svm import SVC import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import roc_curve class_1 = 500 # 类别1有500个样本 class_2 = 50 # 类别2只有50个 centers = [[0.0, 0.0], [2.0, 2.0]] # 设定两个类别的中心 clusters_std = [1.5, 0.5] # 设定两个类别的方差，通常来说，样本量比较大的类别会更加松散 X, y = make_blobs(n_samples=[class_1, class_2], centers=centers, cluster_std=clusters_std, random_state=0, shuffle=False) #看看数据集长什么样 plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"rainbow\",s=10) #其中红色点是少数类，紫色点是多数类 clf_proba = SVC(kernel=\"linear\",C=1.0,probability=True).fit(X,y) FPR, recall, thresholds = roc_curve(y,clf_proba.decision_function(X), pos_label=1) plt.plot(FPR, recall, color='red',label='ROC curve (area = %0.2f)' ) Example 2 初始数据： y_true = [0, 0, 1, 0, 0, 1, 0, 1, 0, 0] y_score = [0.31689620142873609, 0.32367439192936548, 0.42600526758001989, 0.38769987193780364, 0.3667541015524296, 0.39760831479768338, 0.42017521636505745, 0.41936155918127238, 0.33803961944475219, 0.33998332945141224] 通过sklearn的roc_curve函数计算false positive rate和true positive rate以及对应的threshold： fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=False) 计算得到的值如下： fpr array([0. , 0. , 0.14285714, 0.14285714, 0.14285714, 0.28571429, 0.42857143, 0.57142857, 0.71428571, 0.85714286, 1. ]) tpr array([0. , 0.33333333, 0.33333333, 0.66666667, 1. , 1. , 1. , 1. , 1. , 1. , 1. ]) thresholds array([1.42600527, 0.42600527, 0.42017522, 0.41936156, 0.39760831, 0.38769987, 0.3667541 , 0.33998333, 0.33803962, 0.32367439, 0.3168962 ]) Update time： 2020-07-09 "},"Chapter2/ROC曲线下的面积auc.html":{"url":"Chapter2/ROC曲线下的面积auc.html","title":"ROC曲线下的面积auc","keywords":"","body":"ROC曲线下的面积auc sklearn.metrics.auc(x, y) Compute Area Under the Curve (AUC) using the trapezoidal rule Parameters: x array, shape = [n] x coordinates. These must be either monotonic increasing or monotonic decreasing. y array, shape = [n] y coordinates. Returns: auc Examples >>> import numpy as np >>> from sklearn import metrics >>> y = np.array([1, 1, 2, 2]) >>> pred = np.array([0.1, 0.4, 0.35, 0.8]) >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2) >>> metrics.auc(fpr, tpr) 0.75 from sklearn.metrics import roc_curve,auc y_true = [0, 0, 1, 0, 0, 1, 0, 1, 0, 0] y_score = [0.31689620142873609, 0.32367439192936548, 0.42600526758001989, 0.38769987193780364, 0.3667541015524296, 0.39760831479768338, 0.42017521636505745, 0.41936155918127238, 0.33803961944475219, 0.33998332945141224] fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=False) auc(fpr,tpr) # 0.9047619047619048 Update time： 2020-07-09 "},"Chapter2/sklearn.metrics.roc_auc_score用法.html":{"url":"Chapter2/sklearn.metrics.roc_auc_score用法.html","title":"sklearn.metrics.roc_auc_score用法","keywords":"","body":"sklearn.metrics.roc_auc_score用法 计算AUC (Area Under Curve) 面积的类 sklearn.metrics.roc_auc_score 直接根据真实值（必须是二值）、预测值（可以是0/1,也可以是proba值）计算出auc值，中间过程的roc计算省略。 sklearn.metrics.roc_auc_score ( y_true, y_score, average=’macro’, sample_weight=None, max_fpr=None ) y_true :array, shape = [n_samples] or [n_samples, n_classes] 真实的标签 y_score :array, shape = [n_samples] or [n_samples, n_classes] 预测得分，可以是正类的估计概率、置信值或者分类器方法 “decision_function” 的返回值； average :string, [None, ‘micro’, ‘macro’ (default), ‘samples’, ‘weighted’] sample_weight : array-like of shape = [n_samples], optional from sklearn.metrics import roc_auc_score as AUC area = AUC(y,clf_proba.decision_function(X)) Update time： 2020-05-23 "},"Chapter3/":{"url":"Chapter3/","title":"数据处理 Preprocessing","keywords":"","body":"数据处理 Preprocessing Update time： 2020-05-23 "},"Chapter3/preprocessing.MinMaxScaler用法.html":{"url":"Chapter3/preprocessing.MinMaxScaler用法.html","title":"preprocessing.MinMaxScaler用法","keywords":"","body":"preprocessing.MinMaxScaler用法 preprocessing.MinMaxScaler 当数据(x)按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到 [0,1] 之间，而这个过程，就叫做数据归一化(Normalization，又称Min-Max Scaling)。注意，Normalization是归一化，不是正则化，真正的正则化是regularization，不是数据预处理的一种手段。归一化之后的数据服从正态分布，公式如下： x∗=x−min(x)max(x)−min(x)\r x^{*}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)}\r x​∗​​=​max(x)−min(x)​​x−min(x)​​ 在sklearn当中，我们使用preprocessing.MinMaxScaler来实现这个功能。MinMaxScaler有一个重要参数，feature_range，控制我们希望把数据压缩到的范围，默认是[0,1] . 函数： class sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True ) 属性： min_ : ， 计算方式 min - X.min(axis=0) * self.scale_ scale_ : 每个特征的相对缩放比例，计算方式 (max - min) / (X.max(axis=0) - X.min(axis=0)) data_min_ ：最小值 data_max_ : 最大值 ata_range_ : 数据的范围 ，计算方式 datamax - datamin 源码：sklearn.preprocessing.MinMaxScaler from sklearn.preprocessing import MinMaxScaler data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] import pandas as pd pd.DataFrame(data) #实现归一化 scaler = MinMaxScaler() #实例化 scaler = scaler.fit(data) #fit，在这里本质是生成min(x)和max(x) result = scaler.transform(data) #通过接口导出结果 result result_ = scaler.fit_transform(data) #训练和导出结果一步达成 scaler.inverse_transform(result) #将归一化后的结果逆转 #使用MinMaxScaler的参数feature_range实现将数据归一化到[0,1]以外的范围中 data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] scaler = MinMaxScaler(feature_range=[5,10]) #依然实例化 result = scaler.fit_transform(data) #fit_transform一步导出结果 result #当X中的特征数量非常多的时候，fit会报错并表示，数据量太大了我计算不了 #此时使用partial_fit作为训练接口 #scaler = scaler.partial_fit(data) Update time： 2020-05-23 "},"Chapter3/preprocessing.StandardScaler用法.html":{"url":"Chapter3/preprocessing.StandardScaler用法.html","title":"preprocessing.StandardScaler用法","keywords":"","body":"preprocessing.StandardScaler用法 preprocessing.StandardScaler 当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分布），而这个过程，就叫做数据标准化(Standardization，又称Z-score normalization)，公式如下：x∗=x−μσ\r x^{*}=\\frac{x-\\mu}{\\sigma}\r x​∗​​=​σ​​x−μ​​ 函数： class sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True ) 属性： scale_：标准差 mean_：均值 var_： 方差 StandardScaler和MinMaxScaler选哪个？ 大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为MinMaxScaler对异常值非常敏。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。 MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。 建议先试试看StandardScaler，效果不好换MinMaxScaler Update time： 2020-05-23 "},"Chapter3/preprocessing.Binarizer用法.html":{"url":"Chapter3/preprocessing.Binarizer用法.html","title":"preprocessing.Binarizer用法","keywords":"","body":"preprocessing.Binarizer用法 二值数据是使用值将数据转化为二值，大于阈值设置为1，小于阈值设置为0。这个过程被叫做二分数据或阈值转换。在生成明确值或特征工程增加属性时候使用，使用scikit-learn 中的Binarizer类实现。 sklearn.preprocessing.Binarizer(threshold=0.0, copy=True ) Examples from sklearn.preprocessing import Binarizer X = [[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]] transformer = Binarizer().fit(X) # fit does nothing. transformer transformer.transform(X) 二值化器(binarizer)的阈值是可以被调节的: from sklearn.preprocessing import Binarizer from sklearn import preprocessing X = [[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]] transform = Binarizer(threshold=0.0) newX=transform.fit_transform(X) # print(mm) # transform = Binarizer(threshold=0.0).fit(X) # newX = transform.transform(X) binarizer = preprocessing.Binarizer().fit(X) # fit does nothing print(binarizer) #Binarizer(copy=True, threshold=0.0) print(binarizer.transform(X)) ''' [[1. 0. 1.] [1. 0. 0.] [0. 1. 0.]] ''' binarizer = preprocessing.Binarizer(threshold=1.1) print(binarizer.transform(X)) ''' [[0. 0. 1.] [1. 0. 0.] [0. 0. 0.]] ''' Update time： 2020-05-23 "},"Chapter3/preprocessing.KBinsDiscretizer用法.html":{"url":"Chapter3/preprocessing.KBinsDiscretizer用法.html","title":"preprocessing.KBinsDiscretizer用法","keywords":"","body":"preprocessing.KBinsDiscretizer用法 将连续型变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码 . class sklearn.preprocessing.KBinsDiscretizer(n_bins=5, encode='onehot', strategy='quantile' ) Parameters 参数 含义&输入 n_bins 每个特征中分箱的个数，默认5，一次会被运用到所有导入的特征 encode 编码的方式，默认“onehot” \"onehot\"：做哑变量，之后返回一个稀疏矩阵，每一列是一个特征中的一个类别，含有该 类别的样本表示为1，不含的表示为0 “ordinal”：每个特征的每个箱都被编码为一个整数，返回每一列是一个特征，每个特征下含 有不同整数编码的箱的矩阵 \"onehot-dense\"：做哑变量，之后返回一个密集数组。 strategy 用来定义箱宽的方式，默认\"quantile\" \"uniform\"：表示等宽分箱，即每个特征中的每个箱的最大值之间的差为 (特征.max() - 特征.min())/(n_bins) \"quantile\"：表示等位分箱，即每个特征中的每个箱内的样本数量都相同 \"kmeans\"：表示按聚类分箱，每个箱中的值到最近的一维k均值聚类的簇心得距离都相同 Attributes n_bins_ 每个特征的数量 from sklearn.preprocessing import KBinsDiscretizer X = data.iloc[:,0].values.reshape(-1,1) # array([[1], # [0], # [1], # ..., # ..., # [7], # [6], # [9]], dtype=int64) est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform') est.fit_transform(X) #查看转换后分的箱：变成了一列中的三箱 set(est.fit_transform(X).ravel()) est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform') #查看转换后分的箱：变成了哑变量 est.fit_transform(X).toarray() # array([[1., 0., 0.], # [0., 1., 0.], # [1., 0., 0.], # ..., # [0., 1., 0.], # [1., 0., 0.], # [0., 1., 0.]]) Update time： 2020-05-23 "},"Chapter3/preprocessing.LabelEncoder用法.html":{"url":"Chapter3/preprocessing.LabelEncoder用法.html","title":"preprocessing.LabelEncoder用法","keywords":"","body":"preprocessing.LabelEncoder用法 在机器学习中，大多数算法，譬如逻辑回归，支持向量机SVM，k近邻算法等都只能够处理数值型数据，不能处理文字，在sklearn当中，除了专用来处理文字的算法，其他算法在fit的时候全部要求输入数组或矩阵，也不能够导入文字型数据（其实手写决策树和普斯贝叶斯可以处理文字，但是sklearn中规定必须导入数值型） 然而在现实中，许多标签和特征在数据收集完毕的时候，都不是以数字来表现的。比如说，学历的取值可以是[\"小学\"，“初中”，“高中”，\"大学\"]，付费方式可能包含[\"支付宝\"，“现金”，“微信”]等等。在这种情况下，为了让数据适应算法和库，我们必须将数据进行编码，即是说，将文字型数据转换为数值型 class sklearn.preprocessing.LabelEncoder Attributes： classes_ array of shape (n_class,) Holds the label for each class Methods fit(self, y) Fit label encoder fit_transform(self, y) Fit label encoder and return encoded labels get_params(self[, deep]) Get parameters for this estimator. inverse_transform(self, y) Transform labels back to original encoding. transform(self, y) Transform labels to normalized encoding. LabelEncoder 可以将标签分配一个0—n_classes-1之间的编码将各种标签分配一个可数的连续编号： Example from sklearn import preprocessing le = preprocessing.LabelEncoder() lb=le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) lb.classes_ # array(['amsterdam', 'paris', 'tokyo'], dtype=' Update time： 2020-05-23 "},"Chapter3/preprocessing.OneHotEncoder用法.html":{"url":"Chapter3/preprocessing.OneHotEncoder用法.html","title":"preprocessing.OneHotEncoder用法","keywords":"","body":"preprocessing.OneHotEncoder用法 在 sklearn 包中，OneHotEncoder 函数非常实用，它可以实现将分类特征的每个元素转化为一个可以用来计算的值。 class sklearn.preprocessing.OneHotEncoder(categories='auto', drop=None, sparse=True, dtype=, handle_unknown='error' ) Parameters sparse=True 、 表示编码的格式，默认为 True，即为稀疏的格式，指定 False 则就不用 toarray() 了 handle_unknown=’error’ 其值可以指定为 \"error\" 或者 \"ignore\"，即如果碰到未知的类别，是返回一个错误还是忽略它。 Attributes categories_ 查看特征中的类别 在实际的机器学习的应用任务中，特征有时候并不总是连续值，有可能是一些分类值，如性别可分为“male”和“female”。在机器学习任务中，对于这样的特征，通常我们需要对其进行特征数字化，如下面的例子： 性别：[\"male\"，\"female\"] 地区：[\"Europe\"，\"US\"，\"Asia\"] 浏览器：[\"Firefox\"，\"Chrome\"，\"Safari\"，\"Internet Explorer\"] 对于某一个样本，如[\"male\"，\"US\"，\"Internet Explorer\"]，我们需要将这个分类值的特征数字化，最直接的方法，我们可以采用序列化的方式：[0,1,3]。但是这样的特征处理并不能直接放入机器学习算法中。 对于上述的问题，性别的属性是二维的，同理，地区是三维的，浏览器则是思维的，这样，我们可以采用One-Hot编码的方式对上述的样本“[\"male\"，\"US\"，\"Internet Explorer\"]”编码，“male”则对应着[1，0]，同理“US”对应着[0，1，0]，“Internet Explorer”对应着[0,0,0,1]。则完整的特征数字化的结果为：[1,0,0,1,0,0,0,0,1]。这样导致的一个结果就是数据会变得非常的稀疏。 Examples from sklearn.preprocessing import OneHotEncoder enc = OneHotEncoder(sparse = False) ans = enc.fit_transform([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) from sklearn.preprocessing import OneHotEncoder enc = OneHotEncoder(sparse = False) ans = enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) enc.categories_ # [array([0., 1.]), array([0., 1., 2.]), array([0., 1., 2., 3.])] 编码与哑变量 Update time： 2020-05-23 "},"Chapter4/":{"url":"Chapter4/","title":"缺失值处理 sklearn.impute","keywords":"","body":"缺失值处理 sklearn.inspection Update time： 2020-05-23 "},"Chapter4/impute.SimpleImputer用法.html":{"url":"Chapter4/impute.SimpleImputer用法.html","title":"impute.SimpleImputer用法","keywords":"","body":"impute.SimpleImputer用法 sklearn.impute.SimpleImputer (missing_values=nan, strategy=’mean’, fill_value=None, verbose=0, copy=True ) 这个类是专门用来填补缺失值的。它包括四个重要参数： missing_values 告诉SimpleImputer，数据中的缺失值长什么样，默认空值np.nan strategy 填补缺失值的策略，默认均值 输入“mean”使用均值填补（仅对数值型特征可用） 输入“median\"用中值填补（仅对数值型特征可用） 输入\"most_frequent”用众数填补（对数值型和字符型特征都可用） 输入“constant\"表示请参考参数“fill_value\"中的值（对数值型和字符型特征都可用） fill_value 当参数 startegy 为 ”constant\" 的时候可用，可输入字符串或数字表示要填充的值，常用 0 Example import pandas as pd data = pd.read_csv(r\"Narrativedata.csv\",index_col=0) data.head() Age Sex Embarked Survived 0 22.0 male S No 1 38.0 female C Yes 2 26.0 female S Yes 3 35.0 female S Yes 4 35.0 male S No data.info() Int64Index: 891 entries, 0 to 890 Data columns (total 4 columns): Age 714 non-null float64 Sex 891 non-null object Embarked 889 non-null object Survived 891 non-null object dtypes: float64(1), object(3) memory usage: 34.8+ KB Age = data.loc[:,\"Age\"].values.reshape(-1,1) #sklearn当中特征矩阵必须是二维 from sklearn.impute import SimpleImputer imp_mean = SimpleImputer() #实例化，默认均值填补 imp_median = SimpleImputer(strategy=\"median\") #用中位数填补 imp_0 = SimpleImputer(strategy=\"constant\",fill_value=0) #用0填补 imp_mean = imp_mean.fit_transform(Age) #fit_transform一步完成调取结果 imp_median = imp_median.fit_transform(Age) imp_0 = imp_0.fit_transform(Age) imp_mean[:20] imp_median[:20] imp_0[:20] #在这里我们使用中位数填补Age data.loc[:,\"Age\"] = imp_median data.info() #使用众数填补Embarked Embarked = data.loc[:,\"Embarked\"].values.reshape(-1,1) imp_mode = SimpleImputer(strategy = \"most_frequent\") data.loc[:,\"Embarked\"] = imp_mode.fit_transform(Embarked) data.info() BONUS：用Pandas和Numpy进行填补其实更加简单 import pandas as pd data = pd.read_csv(r\"Narrativedata.csv\",index_col=0) data.head() data.loc[:,\"Age\"] = data.loc[:,\"Age\"].fillna(data.loc[:,\"Age\"].median()) #.fillna 在DataFrame里面直接进行填补 data.dropna(axis=0,inplace=True) #.dropna(axis=0)删除所有有缺失值的行，.dropna(axis=1)删除所有有缺失值的列 #参数inplace，为True表示在原数据集上进行修改，为False表示生成一个复制对象，不修改原数据，默认False Update time： 2020-05-23 "},"Chapter5/":{"url":"Chapter5/","title":"特征选择 feature_selection","keywords":"","body":"特征选择 feature_selection Update time： 2020-05-23 "},"Chapter5/feature_selection.VarianceThreshold 用法.html":{"url":"Chapter5/feature_selection.VarianceThreshold 用法.html","title":"feature_selection.VarianceThreshold用法","keywords":"","body":"feature_selection.VarianceThreshold用法 这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为0的特征。VarianceThreshold有重要参数threshold，表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0，即删除所有的记录都相同的特征 class sklearn.feature_selection.VarianceThreshold(threshold=0.0) Attributes variances_ array, shape (n_features,) Variances of individual features. from sklearn.feature_selection import VarianceThreshold selector = VarianceThreshold() #实例化，不填参数默认方差为0 X_var0 = selector.fit_transform(X) #获取删除不合格特征之后的新特征矩阵 #也可以直接写成 X = VairanceThreshold().fit_transform(X) >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]] >>> selector = VarianceThreshold() >>> selector.fit_transform(X) # 经过过滤，保留了两个特征 array([[2, 0], [1, 4], [1, 1]]) Update time： 2020-05-23 "},"Chapter5/feature_selection.SelectKBest用法.html":{"url":"Chapter5/feature_selection.SelectKBest用法.html","title":"feature_selection.SelectKBest用法","keywords":"","body":"feature_selection.SelectKBest用法 根据评分，选取的评分较高的 k 个特征。 class sklearn.feature_selection.SelectKBest(score_func=, k=10 ) Parameters score_func callable 函数接受两个数组X和y，并返回一对数组（分数，pvalue）或带分数的单个数组。 k int or “all”, optional, default=10 Number of top features to select Attributes scores_ array-like of shape (n_features,) Scores of features. pvalues_ array-like of shape (n_features,) p-values of feature scores, None if score_func returned only scores. Methods fit(self, X, y) Run score function on (X, y) and get the appropriate features. fit_transform(self, X[, y]) Fit to data, then transform it. get_params(self[, deep]) Get parameters for this estimator. get_support(self[, indices]) Get a mask, or integer index, of the features selected inverse_transform(self, X) Reverse the transformation operation set_params(self, **params) Set the parameters of this estimator. transform(self, X) Reduce X to the selected features. Update time： 2020-05-23 "},"Chapter5/feature_selection.chi2用法.html":{"url":"Chapter5/feature_selection.chi2用法.html","title":"feature_selection.chi2用法","keywords":"","body":"feature_selection.chi2用法 方差挑选完毕之后，我们就要考虑下一个问题：相关性了。我们希望选出与标签相关且有意义的特征，因为这样的特征能够为我们提供大量信息。如果特征与标签无关，那只会白白浪费我们的计算内存，可能还会给模型带来噪音。 在sklearn当中，我们有三种常用的方法来评判特征与标签之间的相关性：卡方，F检验，互信息 卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。卡方检验类feature_selection.chi2计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合feature_selection.SelectKBest这个可以输入”评分标准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征 另外，如果卡方检验检测到某个特征中所有的值都相同，会提示我们使用方差先进行方差过滤。并且，刚才我们已经验证过，当我们使用方差过滤筛选掉一半的特征后，模型的表现时提升的。因此在这里，我们使用threshold=中位数时完成的方差过滤的数据来做卡方检验（如果方差过滤后模型的表现反而降低了，那我们就不会使用方差过滤后的数据，而是使用原数据） sklearn.feature_selection.chi2(X, y) Parameters X {array-like, sparse matrix} of shape (n_samples, n_features)* Sample vectors. y array-like of shape (n_samples,)* Target vector (class labels). Returns chi2 array, shape = (n_features,)* chi2 statistics of each feature. pval array, shape = (n_features,)* p-values of each feature. 卡方检验的本质是推测两组数据之间的差异，其检验的原假设是”两组数据是相互独立的”。卡方检验返回卡方值和P值两个统计量，其中卡方值很难界定有效的范围，而p值，我们一般使用0.01或0.05作为显著性水平，即p值判断的边界，具体我们可以这样来看 P值 >0.05或0.01 数据差异 差异不是自然形成的 这些差异是很自然的样本误差 相关性 两组数据是相关的 两组数据是相互独立的 原假设 拒绝原假设，接受备择假设 接受原假设 Examples 首先import包和实验数据： from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 from sklearn.datasets import load_iris #导入IRIS数据集 iris = load_iris() iris.data#查看数据 array([[ 5.1, 3.5, 1.4, 0.2], [ 4.9, 3. , 1.4, 0.2], [ 4.7, 3.2, 1.3, 0.2], [ 4.6, 3.1, 1.5, 0.2], [ 5. , 3.6, 1.4, 0.2], [ 5.4, 3.9, 1.7, 0.4], [ 4.6, 3.4, 1.4, 0.3], 使用卡方检验来选择特征 model1 = SelectKBest(chi2, k=2) # 选择k个最佳特征 # iris.data是特征数据，iris.target是标签数据，该函数可以选择出k个特征 model1.fit_transform(iris.data, iris.target) 结果输出为： array([[ 1.4, 0.2], [ 1.4, 0.2], [ 1.3, 0.2], [ 1.5, 0.2], [ 1.4, 0.2], [ 1.7, 0.4], [ 1.4, 0.3], 可以看出后使用卡方检验，选择出了后两个特征。如果我们还想查看卡方检验的p值和得分 model1.scores_ #得分 得分输出为： array([ 10.81782088, 3.59449902, 116.16984746, 67.24482759]) 可以看出后两个特征得分最高，与我们第二步的结果一致； model1.pvalues_ #p-values p值输出为： array([ 4.47651499e-03, 1.65754167e-01, 5.94344354e-26, 2.50017968e-15]) 可以看出后两个特征的p值最小，置信度也最高. Update time： 2020-05-23 "},"Chapter5/feature_selection.f_classif用法.html":{"url":"Chapter5/feature_selection.f_classif用法.html","title":"feature_selection.f_classif用法","keywords":"","body":"feature_selection.f_classif 用法 F 检验，又称 ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归也可以做分类，因此包含feature_selection.f_classif（F检验分类）和feature_selection.f_regression（F检验回归）两个类。其中F检验分类用于标签是离散型变量的数据，而 F检验回归用于标签是连续型变量的数据 。 和卡方检验一样，这两个类需要和类SelectKBest连用，并且我们也可以直接通过输出的统计量来判断我们到底要设置一个什么样的 K 。需要注意的是，F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我们会先将数据转换成服从正态分布的方式 。 sklearn.feature_selection.f_classif(X, y) Parameters X {array-like, sparse matrix} shape = [n_samples, n_features] The set of regressors that will be tested sequentially. y array of shape(n_samples) The data matrix. Returns F array, shape = [n_features,] The set of F values. pval array, shape = [n_features,] The set of p-values. Update time： 2020-05-23 "},"Chapter5/feature_selection.f_regression用法.html":{"url":"Chapter5/feature_selection.f_regression用法.html","title":"feature_selection.f_regression用法","keywords":"","body":"feature_selection.f_regression用法 feature_selection.f_regression 用法 sklearn.feature_selection.f_classif(X, y) Parameters X {array-like, sparse matrix} shape = [n_samples, n_features] The set of regressors that will be tested sequentially. y array of shape(n_samples) The data matrix. center rue, bool, If true, X and y will be centered. Returns F array, shape = [n_features,] F values of features. pval array, shape = [n_features,] p-values of F-scores. Update time： 2020-05-23 "},"Chapter5/feature_selection.mutual_info_classif用法.html":{"url":"Chapter5/feature_selection.mutual_info_classif用法.html","title":"feature_selection.mutual_info_classif用法","keywords":"","body":"feature_selection.mutual_info_classif用法 互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既 可以做回归也可以做分类，并且包含两个类 feature_selection.mutual_info_classif（互信息分类）和 feature_selection.mutual_info_regression（互信息回归）。这两个类的用法和参数都和F检验一模一样，不过互信息法比F检验更加强大，F检验只能够找出线性关系，而互信息法可以找出任意关系 sklearn.feature_selection.mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None ) X :特征矩阵 y :目标向量 discrete_features : {‘auto’, bool, array_like}, default ‘auto’ Update time： 2020-05-23 "},"Chapter5/feature_selection.mutual_info_regression用法.html":{"url":"Chapter5/feature_selection.mutual_info_regression用法.html","title":"feature_selection.mutual_info_regression用法","keywords":"","body":"feature_selection.mutual_info_regression用法 sklearn.feature_selection.mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None ) X :特征矩阵 y :目标向量 discrete_features : {‘auto’, bool, array_like}, default ‘auto’ Update time： 2020-05-23 "},"Chapter5/过滤法总结.html":{"url":"Chapter5/过滤法总结.html","title":"过滤法总结","keywords":"","body":"过滤法总结 常用的基于过滤法的特征选择，包括方差过滤，基于卡方，F检验和互信息的相关性过滤 , 先使用方差过滤，然后使用互信息法来捕捉相关性，不过了解各种各样的过滤方式也是必要的。 类 说明 超参数的选择 VarianceThreshold 方差过滤，可输入方差阈值，返回方差大于 阈值的新特征矩阵 看具体数据究竟是含有更多噪 声还是更多有效特征 一般就使用0或1来筛选 也可以画学习曲线或取中位数 跑模型来帮助确认 SelectKBest 用来选取K个统计量结果最佳的特征，生成 符合统计量要求的新特征矩阵 看配合使用的统计量 chi2 卡方检验，专用于分类算法，捕捉相关性 追求p小于显著性水平的特征 f_classif F检验分类，只能捕捉线性相关性 要求数据服从正态分布 追求p小于显著性水平的特征 f_regression F检验回归，只能捕捉线性相关性 要求数据服从正态分布 追求p小于显著性水平的特征 mutual_info_classif 互信息分类，可以捕捉任何相关性 不能用于稀疏矩阵 追求互信息估计大于0的特征 mutual_info_regression 互信息回归，可以捕捉任何相关性 不能用于稀疏矩阵 追求互信息估计大于0的特征 Update time： 2020-05-23 "},"Chapter5/feature_selection.SelectFromModel用法.html":{"url":"Chapter5/feature_selection.SelectFromModel用法.html","title":"feature_selection.SelectFromModel用法","keywords":"","body":"feature_selection.SelectFromModel用法 Embedded嵌入法 嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系数往往代表了特征对于模型的某种贡献或某种重要性，比如决策树和树的集成模型中的feature_importances_属性，可以列出各个特征对树的建立的贡献，我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。因此相比于过滤法，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果 。 过滤法中使用的统计量可以使用统计知识和常识来查找范围（如p值应当低于显著性水平0.05），而嵌入法中使用的权值系数却没有这样的范围可找——我们可以说，权值系数为0的特征对模型丝毫没有作用，但当大量特征都对模型有贡献且贡献不一时，我们就很难去界定一个有效的临界值。这种情况下，模型权值系数就是我们的超参数，我们或许需要学习曲线，或者根据模型本身的某些性质去判断这个超参数的最佳值究竟应该是多少。 feature_selection.SelectFromModel lass sklearn.feature_selection.SelectFromModel (estimator, threshold=None, prefit=False, norm_order=1, max_features=None ) SelectFromModel是一个元变换器，可以与任何在拟合后具有coef_，feature_importances_属性或参数中可选惩罚项的评估器一起使用（比如随机森林和树模型就具有属性feature_importances_，逻辑回归就带有l1和l2惩罚项，线性支持向量机也支持l2惩罚项） 对于有feature_importances_的模型来说，若重要性低于提供的阈值参数，则认为这些特征不重要并被移除。feature_importances_的取值范围是[0,1]，如果设置阈值很小，比如0.001，就可以删除那些对标签预测完全没贡献的特征。如果设置得很接近1，可能只有一两个特征能够被留下 参数 说明 estimator 使用的模型评估器，只要是带featureimportances或者coef_属性，或带有l1和l2惩罚 项的模型都可以使用 threshold 特征重要性的阈值，重要性低于这个阈值的特征都将被删除 prefit 默认False，判断是否将实例化后的模型直接传递给构造函数。如果为True，则必须直接 调用fit和transform，不能使用fit_transform，并且SelectFromModel不能与 cross_val_score，GridSearchCV和克隆估计器的类似实用程序一起使用。 norm_order k可输入非零整数，正无穷，负无穷，默认值为1 在评估器的coef_属性高于一维的情况下，用于过滤低于阈值的系数的向量的范数的阶 数。 max_features 在阈值设定下，要选择的最大特征数。要禁用阈值并仅根据max_features选择，请设置 threshold = -np.inf from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier as RFC RFC_ = RFC(n_estimators =10,random_state=0) X_embedded = SelectFromModel(RFC_,threshold=0.005).fit_transform(X,y) #在这里我只想取出来有限的特征。0.005这个阈值对于有780个特征的数据来说，是非常高的阈值，因为平均每个特征 #只能够分到大约0.001的feature_importances_ X_embedded.shape #模型的维度明显被降低了 #同样的，我们也可以画学习曲线来找最佳阈值 #======【TIME WARNING：10 mins】======# import numpy as np import matplotlib.pyplot as plt RFC_.fit(X,y).feature_importances_ threshold = np.linspace(0,(RFC_.fit(X,y).feature_importances_).max(),20) score = [] for i in threshold: X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y) once = cross_val_score(RFC_,X_embedded,y,cv=5).mean() score.append(once) plt.plot(threshold,score) plt.show() Update time： 2020-05-23 "},"Chapter5/feature_selection.RFE用法.html":{"url":"Chapter5/feature_selection.RFE用法.html","title":"feature_selection.RFE用法","keywords":"","body":"feature_selection.RFE用法 包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如 coef_属性或 feature_importances_ 属性来完成特征选择。但不同的是，我们往往使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。包装法在初始特征集上训练评估器，并且通过coef_属性或通过feature_importances_ 属性获得每个特征的重要性。然后，从当前的一组特征中修剪最不重要的特征。在修剪的集合上递归地重复该过程，直到最终到达所需数量的要选择的特征。区别于过滤法和嵌入法的一次训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。 注意，在这个图中的“算法”，指的不是我们最终用来导入数据的分类或回归算法（即不是随机森林），而是专业的据挖掘算法，即我们的目标函数。这些数据挖掘算法的核心功能就是选取最佳特征子集。 最典型的目标函数是递归特征消除法（Recursive feature elimination, 简写为RFE）。它是一种贪婪的优化算法，旨在找到性能最佳的特征子集。 它反复创建模型，并在每次迭代时保留最佳特征或剔除最差特征，下一次迭代时，它会使用上一次建模中没有被选中的特征来构建下一个模型，直到所有特征都耗尽为止。 然后，它根据自己保留或剔除特征的顺序来对特征进行排名，最终选出一个最佳子集。 包装法的效果是所有特征选择方法中最利于提升模型表现的，它可以使用很少的特征达到很优秀的效果。除此之外，在特征数目相同时，包装法和嵌入法的效果能够匹敌，不过它比嵌入法算得更见缓慢，所以也不适用于太大型的数据。相比之下，包装法是最能保证模型效果的特征选择方法 class sklearn.feature_selection.RFE (estimator, n_features_to_select=None, step=1, verbose=0 ) 参数estimator是需要填写的实例化后的评估器，n_features_to_select是想要选择的特征个数，step表示每次迭代中希望移除的特征个数。除此之外，RFE类有两个很重要的属性，.support_：返回所有的特征的是否最后被选中的布尔矩阵，以及 .ranking_ 返回特征的按数次迭代中综合重要性的排名 类feature_selection.RFECV会在交叉验证循环中执行RFE以找到最佳数量的特征，增加参数cv，其他用法都和RFE一模一样 from sklearn.feature_selection import RFE RFC_ = RFC(n_estimators =10,random_state=0) selector = RFE(RFC_, n_features_to_select=340, step=50).fit(X, y) selector.support_.sum() selector.ranking_ X_wrapper = selector.transform(X) cross_val_score(RFC_,X_wrapper,y,cv=5).mean() #======【TIME WARNING: 15 mins】======# score = [] for i in range(1,751,50): X_wrapper = RFE(RFC_,n_features_to_select=i, step=50).fit_transform(X,y) once = cross_val_score(RFC_,X_wrapper,y,cv=5).mean() score.append(once) plt.figure(figsize=[20,5]) plt.plot(range(1,751,50),score) plt.xticks(range(1,751,50)) plt.show() 明显能够看出，在包装法下面，应用50个特征时，模型的表现就已经达到了90%以上，比嵌入法和过滤法都高效很多。我们可以放大图像，寻找模型变得非常稳定的点来画进一步的学习曲线（就像我们在嵌入法中做的那样）。如果我们此时追求的是最大化降低模型的运行时间，我们甚至可以直接选择50作为特征的数目，这是一个在缩减了94%的特征的基础上，还能保证模型表现在90%以上的特征组合，不可谓不高效 Update time： 2020-05-23 "},"model_selection/":{"url":"model_selection/","title":"模型选择 model_selection","keywords":"","body":"模型选择 model_selection Update time： 2020-07-08 "},"model_selection/数据集划分train_test_split.html":{"url":"model_selection/数据集划分train_test_split.html","title":"数据集划分train_test_split","keywords":"","body":"数据集划分train_test_split train_test_split 是交叉验证中常用的函数，功能是从样本中随机的按比例选取train_data和test_data，形式为： sklearn.model_selection.train_test_split(train_data, train_target, test_siz=, random_state=, shuffle) 参数 train_data：所要划分的样本特征集 train_target：所要划分的样本结果 test_size：样本占比，如果是整数的话就是样本的数量 random_state：是随机数的种子。 shuffle : bool, default=True 返回值 ：划分后的数据集 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。 随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则： 种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。 案例 import numpy as np from sklearn.model_selection import train_test_split X = np.array([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) y = [0, 1, 2, 3, 4] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=0) Update time： 2020-07-08 "},"model_selection/数据集划分K折交叉验证KFold.html":{"url":"model_selection/数据集划分K折交叉验证KFold.html","title":"数据集划分K折交叉验证KFold","keywords":"","body":"数据集划分K折交叉验证KFold 将全部训练集S分成k个不相交的子集，假设S中的训练样例个数为m，那么每一个自己有m/k个训练样例，相应的子集为{s1，s2，...，sk} 每次从分好的子集里面，拿出一个作为测试集，其他k-1个作为训练集 在k-1个训练集上训练出学习器模型 把这个模型放到测试集上，得到分类率的平均值，作为该模型或者假设函数的真实分类率 这个方法充分利用了所以样本，但计算比较繁琐，需要训练k次，测试k次 函数 class sklearn.model_selection.KFold(n_splits=5, *, shuffle=False, random_state=None) 参数： n_splits：表示划分几等份 shuffle：在每次划分时，是否进行洗牌 若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同 若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的 random_state：随机种子数 Methods get_n_splits(X=None, y=None, groups=None)：获取参数n_splits的值 split(X, y=None, groups=None)：将数据集划分成训练集和测试集，返回索引生成器 Examples >>> import numpy as np >>> from sklearn.model_selection import KFold >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) >>> y = np.array([1, 2, 3, 4]) >>> kf = KFold(n_splits=2) >>> kf.get_n_splits(X) 2 >>> print(kf) KFold(n_splits=2, random_state=None, shuffle=False) # kf.split(X) 获取划分后的索引 >>> for train_index, test_index in kf.split(X): ... print(\"TRAIN:\", train_index, \"TEST:\", test_index) ... X_train, X_test = X[train_index], X[test_index] ... y_train, y_test = y[train_index], y[test_index] TRAIN: [2 3] TEST: [0 1] TRAIN: [0 1] TEST: [2 3] 参考 KFold sklearn中的数据集的划分 sklearn.model_selection.KFold 【机器学习】Cross-Validation（交叉验证）详解 Update time： 2020-07-09 "},"model_selection/数据集划分K折交叉验证StratifiedKFold.html":{"url":"model_selection/数据集划分K折交叉验证StratifiedKFold.html","title":"数据集划分K折交叉验证StratifiedKFold","keywords":"","body":"数据集划分K折交叉验证StratifiedKFold StratifiedKFold 用法类似Kfold，但是它是分层采样，确保训练集，验证集中各类别样本的比例与原始数据集中相同。因此一般使用 StratifiedKFold。保证训练集中每一类的比例是相同的 class sklearn.model_selection.StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None) 参数 n_splits:折叠次数，默认为3，至少为2。 shuffle: 是否在每次分割之前打乱顺序。 random_state: 随机种子，在 shuffle==True 时使用，默认使用 np.random。 Methods get_n_splits(self, X=None, y=None, groups=None) StratifiedKFold.split(X, y, groups=None) 参数： X :array-like,shape(n_sample,n_features)，训练数据集。 y:array-like,shape(n_sample)，标签。 返回值：训练集数据的 index 与验证集数据的 index。 from sklearn.model_selection import StratifiedKFold X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6], [7, 8]]) y = np.array([0, 0, 0, 1, 1, 1]) # 1的个数和0的个数要大于3，3也就是n_splits skf = StratifiedKFold(n_splits=3) for train_index, test_index in skf.split(X, y): print(\"TRAIN:\", train_index, \"TEST:\", test_index) 结果： TRAIN: [1 2 4 5] TEST: [0 3] TRAIN: [0 2 3 5] TEST: [1 4] TRAIN: [0 1 3 4] TEST: [2 5] 案例 # 加载数据 from sklearn.preprocessing import LabelEncoder df = pd.read_csv( \"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header=None) # 做基本的数据预处理 X = df.iloc[:, 2:].values y = df.iloc[:, 1].values le = LabelEncoder() # 将M-B等字符串编码成计算机能识别的0-1 y = le.fit_transform(y) le.transform(['M', 'B']) # 数据切分8：2 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=1) # 分层 k折交叉验证 from sklearn.model_selection import StratifiedKFold kfold = StratifiedKFold(n_splits=10, random_state=1).split(X_train, y_train) scores2 = [] for k, (train, test) in enumerate(kfold): pipe_lr1.fit(X_train[train], y_train[train]) score = pipe_lr1.score(X_train[test], y_train[test]) scores2.append(score) print('Fold:%2d,Class dist.:%s,Acc:%.3f' % (k+1, np.bincount(y_train[train]), score)) print('\\nCV accuracy :%.3f +/-%.3f' % (np.mean(scores2), np.std(scores2))) 结果： Fold: 1,Class dist.:[256 153],Acc:0.935 Fold: 2,Class dist.:[256 153],Acc:0.935 Fold: 3,Class dist.:[256 153],Acc:0.957 Fold: 4,Class dist.:[256 153],Acc:0.957 Fold: 5,Class dist.:[256 153],Acc:0.935 Fold: 6,Class dist.:[257 153],Acc:0.956 Fold: 7,Class dist.:[257 153],Acc:0.978 Fold: 8,Class dist.:[257 153],Acc:0.933 Fold: 9,Class dist.:[257 153],Acc:0.956 Fold:10,Class dist.:[257 153],Acc:0.956 CV accuracy :0.950 +/-0.014 参考 Sklearn中的f1_score与StratifiedKFold sklearn.model_selection.StratifiedKFold sklearn.model_selection的StratifiedKFold实例 Update time： 2020-07-09 "},"model_selection/随机数据集划分ShuffleSplit.html":{"url":"model_selection/随机数据集划分ShuffleSplit.html","title":"随机数据集划分ShuffleSplit","keywords":"","body":"随机数据集划分ShuffleSplit ShuffleSplit() 随机排列交叉验证，生成一个用户给定数量的独立的训练/测试数据划分。样例首先被打散然后划分为一对训练测试集合。 ShuffleSplit为一个迭代器，ShuffleSplit迭代器产生指定数量的独立的train/test数据集划分，首先对样本全体随机打乱，然后再划分出train/test对，可以使用随机数种子random_state来控制数字序列发生器使得讯算结果可重现 ShuffleSplit是KFlod交叉验证的比较好的替代，他允许更好的控制迭代次数和train/test的样本比例 函数 class sklearn.model_selection.ShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None) 参数 n_splits:划分训练集、测试集的次数，默认为10 test_size: 测试集比例或样本数量， 如果是float类型的数据, 这个数应该介于0-1.0之间，代表test集所占比例. 如果是int类型, 代表test集的数量. 如果为None, 值将自动设置为train集大小的补集 train_size : float, int, or None (default is None) 如果是float类型的数据 应该介于0和1之间，并表示数据集在train集分割中所占的比例 如果是int类型, 代表train集的样本数量. 如果为None, 值将自动设置为test集大小的补集 random_state:随机种子值，默认为None，可以通过设定明确的random_state，使得伪随机生成器的结果可以重复。 Methods get_n_splits(self[, X, y, groups]) 返回交叉验证程序中拆分迭代的次数 split(self, X[, y, groups]) 生成索引，将数据拆分为训练集和测试集。 参数： X训练数据，其中n_samples是样本数，n_features是特征数。 y有监督学习问题的目标变量。 groups将数据集拆分为列车/测试集时使用的样本的分组标签。 返回值： trainndarray The training set indices for that split. testndarray The testing set indices for that split. 案例 import numpy as np from sklearn.model_selection import ShuffleSplit X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]]) y = np.array([1, 2, 1, 2, 1, 2]) rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0) rs.get_n_splits(X) 结果：5 print(rs) #结果： ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None) 使用 split() 获取划分后的索引值 for train_index, test_index in rs.split(X): print(\"TRAIN:\", train_index, \"TEST:\", test_index) 结果 TRAIN: [1 3 0 4] TEST: [5 2] TRAIN: [4 0 2 5] TEST: [1 3] TRAIN: [1 2 4 0] TEST: [3 5] TRAIN: [3 4 1 0] TEST: [5 2] TRAIN: [3 5 1 0] TEST: [2 4] 指定不同的训练和测试数据集的比例 rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25, random_state=0) for train_index, test_index in rs.split(X): print(\"TRAIN:\", train_index, \"TEST:\", test_index) 结果 TRAIN: [1 3 0] TEST: [5 2] TRAIN: [4 0 2] TEST: [1 3] TRAIN: [1 2 4] TEST: [3 5] TRAIN: [3 4 1] TEST: [5 2] TRAIN: [3 5 1] TEST: [2 4] 参考 python中shuffleSplit（）函数 sklearn.model_selection.``ShuffleSplit Update time： 2020-07-09 "},"model_selection/随机分层数据集划分StratifiedShuffleSplit.html":{"url":"model_selection/随机分层数据集划分StratifiedShuffleSplit.html","title":"随机分层数据集划分StratifiedShuffleSplit","keywords":"","body":"随机分层数据集划分StratifiedShuffleSplit StratifiedShuffleSplit是ShuffleSplit的一个变体，返回分层划分，也就是在创建划分的时候要保证每一个划分中类的样本比例与整体数据集中的原始比例保持一致 StratifiedShuffleSplit 把数据集打乱顺序，然后划分测试集和训练集，训练集额和测试集的比例随机选定，训练集和测试集的比例的和可以小于1,但是还要保证训练集中各类所占的比例是一样的 函数 class sklearn.model_selection.StratifiedShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None) Methods get_n_splits(self, X=None, y=None, groups=None) split(self, X, y, groups=None) 案例 import numpy as np from sklearn.model_selection import StratifiedShuffleSplit X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]]) y = np.array([0, 0, 0, 1, 1, 1]) sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0) sss.get_n_splits(X, y) 划分的次数 结果：5 print(sss) # StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=0.5, train_size=None) 获取 训练/测试数据集 for train_index, test_index in sss.split(X, y): # 打印划分后的索引 print(\"TRAIN:\", train_index, \"TEST:\", test_index) # 根据索引 获取对应的 训练集和测试集及对应的标签 X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] 结果： TRAIN: [5 2 3] TEST: [4 1 0] TRAIN: [5 1 4] TEST: [0 2 3] TRAIN: [5 0 2] TEST: [4 3 1] TRAIN: [4 1 0] TEST: [2 3 5] TRAIN: [0 5 1] TEST: [3 4 2] 参考 sklearn中的数据集的划分 sklearn.model_selection.StratifiedShuffleSplit Update time： 2020-07-09 "},"model_selection/交叉验证cross_val_score.html":{"url":"model_selection/交叉验证cross_val_score.html","title":"交叉验证cross_val_score","keywords":"","body":"交叉验证cross_val_score k折交叉验证评估模型性能 cross_val_score 交叉验证优点： 交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。 还可以从有限的数据中获取尽可能多的有效信息 model_selection.cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan ) 参数： estimator: 估计方法对象(分类器, 建立好的模型) X：数据特征(Features) y：数据标签(Labels) soring：调用方法(包括accuracy和mean_squared_error等等) cv：几折交叉验证 n_jobs：同时工作的cpu个数（-1代表全部） fit_params：字典，将估计器中fit方法的参数通过字典传 返回值： 每次进行验证的分数列表 scoring参数值解析 sklearn 中的参数 scoring下，均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差 MSE 的数值，其实就是 neg_mean_squared_error 去掉负号的数字。 案例 from sklearn import datasets, linear_model from sklearn.model_selection import cross_val_score import numpy as np diabetes = datasets.load_diabetes() X = diabetes.data[:150] y = diabetes.target[:150] lasso = linear_model.Lasso() scores = cross_val_score(lasso, X, y, cv=3) print(\"CV accuracy scores:%s\" % scores) print(\"CV accuracy:%.3f +/-%.3f\" % (np.mean(scores), np.std(scores))) 结果： CV accuracy scores:[0.33150734 0.08022311 0.03531764] CV accuracy:0.149 +/-0.130 Update time： 2020-07-09 "},"model_selection/学习曲线learning_curve.html":{"url":"model_selection/学习曲线learning_curve.html","title":"学习曲线learning_curve","keywords":"","body":"学习曲线learning_curve learning_curve 是展示不同数据量，算法学习得分 确定交叉验证的针对不同训练集大小的训练和测试分数。 交叉验证生成器将整个数据集拆分为训练和测试数据中的 k 次。 具有不同大小的训练集的子集将用于训练估计器，并为每个训练子集大小和测试集计算分数。 之后，对于每个训练子集大小，将对所有k次运行的得分进行平均。 函数 sklearn.model_selection.learning_curve(estimator, X, y, *, groups=None, train_sizes=array([0.1, 0.33, 0.55, 0.78, 1. ]), cv=None, scoring=None, exploit_incremental_learning=False, n_jobs=None, pre_dispatch='all', verbose=0, shuffle=False, random_state=None, error_score=nan, return_times=False) 参数 estimator：实现“ fit”和“ predict”方法的对象类型 每次验证都会克隆的该类型的对象。 X：数组类，形状（n_samples，n_features） 训练向量，其中n_samples是样本数，n_features是特征数。 y：数组类，形状（n_samples）或（n_samples，n_features），可选 相对于X的目标进行分类或回归；无监督学习无。 groups：数组类，形状为（n_samples，），可选 将数据集拆分为训练/测试集时使用的样本的标签分组。仅用于连接交叉验证实例组（例如GroupKFold）。 train_sizes：数组类，形状（n_ticks），dtype float或int 训练示例的相对或绝对数量，将用于生成学习曲线。如果dtype为float，则视为训练集最大尺寸的一部分（由所选的验证方法确定），即，它必须在（0，1]之内，否则将被解释为绝对大小注意，为了进行分类，样本的数量通常必须足够大，以包含每个类中的至少一个样本（默认值：np.linspace（0.1，1.0，5）） cv：int，交叉验证生成器或可迭代的，可选的 确定交叉验证拆分策略。cv的可能输入是： None，要使用默认的三折交叉验证（v0.22版本中将改为五折） 整数，用于指定（分层）KFold中的折叠数， CV splitter 可迭代的集（训练，测试）拆分为索引数组。 对于整数/无输入，如果估计器是分类器，y是二进制或多类，则使用StratifiedKFold。在所有其他情况下，都使用KFold。 scoring：字符串，可调用或无，可选，默认：None 字符串（参阅model evaluation documentation）或带有签名scorer(estimator, X, y)的计分器可调用对象/函数。 exploit_incremental_learning：布尔值，可选，默认值：False 如果估算器支持增量学习，此参数将用于加快拟合不同训练集大小的速度。 n_jobs：int或None，可选（默认=None） 要并行运行的作业数。None表示1。 -1表示使用所有处理器。有关更多详细信息，请参见词汇表。 pre_dispatch：整数或字符串，可选 并行执行的预调度作业数（默认为全部）。该选项可以减少分配的内存。该字符串可以是“ 2 * n_jobs”之类的表达式。 verbose：整数，可选 控制详细程度：越高，消息越多。 shuffle：布尔值，可选 是否在基于train_sizes为前缀之前对训练数据进行洗牌。 random_state：int，RandomState实例或无，可选（默认=None） 如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果为None，则随机数生成器是np.random使用的RandomState实例。在shuffle为True时使用。 error_score：raise | raise-deprecating 或数字 如果估算器拟合中出现错误，则分配给分数的值。如果设置为“ raise”，则会引发错误。如果设置为“raise-deprecating”，则会在出现错误之前打印FutureWarning。如果给出数值，则引发FitFailedWarning。此参数不会影响重新安装步骤，这将始终引发错误。默认值为“不赞成使用”，但从0.22版开始，它将更改为np.nan。 返回值 train_sizes_abs：数组，形状（n_unique_ticks，），dtype int 已用于生成学习曲线的训练示例数。 请注意，ticks的数量可能少于n_ticks，因为重复的条目将被删除。 train_scores：数组，形状（n_ticks，n_cv_folds） 训练集得分。 test_scores：数组，形状（n_ticks，n_cv_folds） 测试集得分。 案例 from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import warnings import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline plt.style.use(\"ggplot\") warnings.filterwarnings(\"ignore\") # 加载数据 df = pd.read_csv( \"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header=None) # 做基本的数据预处理 X = df.iloc[:, 2:].values y = df.iloc[:, 1].values le = LabelEncoder() # 将M-B等字符串编码成计算机能识别的0-1 y = le.fit_transform(y) le.transform(['M', 'B']) # 数据切分8：2 X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=1) from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline # 用学习曲线诊断偏差与方差 from sklearn.model_selection import learning_curve pipe_lr3 = make_pipeline( StandardScaler(), LogisticRegression(random_state=1, penalty='l2')) train_sizes, train_scores, test_scores = learning_curve( estimator=pipe_lr3, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1, 10), cv=10, n_jobs=1) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) # 在画训练集的曲线时：横轴为 train_sizes，纵轴为 train_scores_mean； # 画测试集的曲线时：横轴为train_sizes，纵轴为test_scores_mean。 plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy') plt.fill_between(train_sizes, train_mean+train_std, train_mean-train_std, alpha=0.15, color='blue') plt.plot(train_sizes, test_mean, color='red', marker='s', markersize=5, label='validation accuracy') plt.fill_between(train_sizes, test_mean+test_std, test_mean-test_std, alpha=0.15, color='red') plt.xlabel(\"Number of training samples\") plt.ylabel(\"Accuracy\") plt.legend(loc='lower right') plt.ylim([0.8, 1.02]) plt.savefig('te.png', dpi=300) 参考 sklearn中的学习曲线learning_curve函数 绘制学习曲线——plot_learning_curve sklearn.model_selection.learning_curve Update time： 2020-07-08 "},"model_selection/验证曲线validation_curve.html":{"url":"model_selection/验证曲线validation_curve.html","title":"验证曲线validation_curve","keywords":"","body":"验证曲线validation_curve validation_curve()的作用 误差是由偏差（bias）、方差（variance）、噪声（noise）组成。 偏差：模型对于不同的训练样本集，预测结果的平均误差 方差：模型对于不同训练样本集的敏感程度 噪声：数据集本身的一项属性 同样的数据，（cos函数上的点加上噪声），我们用同样的模型（polynomial），但是超参数却不同（degree =1,4,15）,会得到不同的拟合效果： 第一个模型太简单，模型本身就拟合不了这些数据（高偏差，underfitting）； 第二个模型可以看成几乎完美地拟合了数据； 第三个模型完美拟合了几乎所有的训练数据，但却不能很好的拟合真实的函数，也就是对于不同的训练数据很敏感（高方差，overfitting）。 对于以上第一和第三个模型，我们可以选择模型和超参数来得到效果更好的配置，也就是可以通过验证曲线（validation_curve）来调节。 validation_curve的含义 证曲线（validation_curve）和学习曲线（sklearn.model_selection.learning_curve()）的区别是，验证曲线的横轴为某个超参数，如一些树形集成学习算法中的max_depth、min_sample_leaf等等。 从验证曲线上可以看到随着超参数设置的改变，模型可能从欠拟合到合适，再到过拟合的过程，进而选择一个合适的位置，来提高模型的性能。 需要注意的是，如果我们使用验证分数来优化超参数，那么该验证分数是有偏差的，它无法再代表魔心的泛化能力，我们就需要使用其他测试集来重新评估模型的泛化能力。 即一般我们需要把一个数据集分成三部分：train、validation和test，我们使用train训练模型，并通过在 validation数据集上的表现不断修改超参数值（例如svm中的C值，gamma值等），当模型超参数在validation数据集上表现最优时，我们再使用全新的测试集test进行测试，以此来衡量模型的泛化能力。 不过有时画出单个超参数与训练分数和验证分数的关系图，有助于观察该模型在该超参数取值时，是否过拟合或欠拟合的情况发生，如下两个图： 如图是SVM在不同gamma时，它在训练集和交叉验证上的分数： gamma很小时，训练分数和验证分数都很低，为欠拟合； gamma逐渐增加时，两个分数都较高，此时模型相对不错； gamma太高时，训练分数高，验证分数低，学习器会过拟合。 本例中，可以选验证集准确率开始下降，而测试集越来越高那个转折点作为gamma的最优选择。 如上图，max_depth的最佳值应该定位5 函数 validation_curve 是展示某个因子，不同取值的算法得分 sklearn.model_selection.validation_curve(estimator, X, y, *, param_name, param_range, groups=None, cv=None, scoring=None, n_jobs=None, pre_dispatch='all', verbose=0, error_score=nan) 参数 estimator : 评估器 X : 训练集 y： 训练集对应的标签 param_name : str ,要改变的参数的名字，如果当model为SVC时，改变gamma的值，求最好的那个gamma值 param_rang: array-like of shape (n_values,) 给定的参数范围 cv : 交叉验证生成器或可迭代的，可选的 确定交叉验证拆分策略。cv的可能输入是： None，要使用默认的 5 折交叉验证 int, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. For int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. scoring: str or callable, default=Non 打分类型，如 accuracy ,r2 等， 返回值： train_scores ：array of shape (n_ticks, n_cv_folds) Scores on training sets. est_scores：array of shape (n_ticks, n_cv_folds) Scores on test set. 案例 1 import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import load_digits from sklearn.svm import SVC from sklearn.model_selection import validation_curve X, y = load_digits(return_X_y=True) param_range = np.logspace(-6, -1, 5) train_scores, test_scores = validation_curve( SVC(), X, y, param_name=\"gamma\", param_range=param_range, scoring=\"accuracy\", n_jobs=1) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.title(\"Validation Curve with SVM\") plt.xlabel(r\"$\\gamma$\") plt.ylabel(\"Score\") plt.ylim(0.0, 1.1) lw = 2 plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw) plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"darkorange\", lw=lw) plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw) plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"navy\", lw=lw) plt.legend(loc=\"best\") 案例 2 带有 cv 参数 import numpy as np import pandas as pd from time import time import matplotlib.pyplot as plt from sklearn.ensemble import GradientBoostingRegressor from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import validation_curve def ModelComplexity(X, y): \"\"\" Calculates the performance of the model as model complexity increases. The learning and testing errors rates are then plotted. \"\"\" # Create 10 cross-validation sets for training and testing cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0) # Vary the max_depth parameter from 1 to 10 max_depth = np.arange(1, 11) start = time() # Calculate the training and testing scores train_scores, test_scores = validation_curve(GradientBoostingRegressor(), X, y, param_name=\"max_depth\", param_range=max_depth, cv=cv, scoring='r2') print(time()-start) # Find the mean and standard deviation for smoothing train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) # Plot the validation curve plt.figure(figsize=(7, 5)) plt.title('Gradient Boosting Regressor Complexity Performance') plt.plot(max_depth, train_mean, 'o-', color='r', label='Training Score') plt.plot(max_depth, test_mean, 'o-', color='g', label='Validation Score') plt.fill_between(max_depth, train_mean - train_std, train_mean + train_std, alpha=0.15, color='r') plt.fill_between(max_depth, test_mean - test_std, test_mean + test_std, alpha=0.15, color='g') # Visual aesthetics plt.legend(loc='lower right') plt.xlabel('Maximum Depth') plt.ylabel('Score') plt.ylim([-0.05, 1.05]) ModelComplexity(X_train, y_train) 画图时，横轴为 param_range，纵轴为 scoreing： 参考 sklearn.model_selection.validation_curve validation_curve验证曲线与超参数 Update time： 2020-07-09 "},"pipeline/":{"url":"pipeline/","title":"pipeline","keywords":"","body":"pipeline Update time： 2020-07-08 "},"pipeline/Pipeline.html":{"url":"pipeline/Pipeline.html","title":"Pipeline","keywords":"","body":"Pipeline 当我们对训练集应用各种预处理操作时（特征标准化、主成分分析等等）， 我们都需要对测试集重复利用这些参数，以免出现数据泄露（data leakage）。 pipeline 实现了对全部步骤的流式化封装和管理（streaming workflows with pipelines），可以很方便地使参数集在新数据集（比如测试集）上被重复使用。 Pipeline可以将许多算法模型串联起来，比如将特征提取、归一化、分类组织在一起形成一个典型的机器学习问题工作流。 pipeline 可以用于下面几处： 模块化 Feature Transform，只需写很少的代码就能将新的 Feature 更新到训练集中。 自动化 Grid Search，只要预先设定好使用的 Model 和参数的候选，就能自动搜索并记录最佳的 Model。 自动化 Ensemble Generation，每隔一段时间将现有最好的 K 个 Model 拿来做 Ensemble。 sklearn.pipeline.make_pipeline(*steps, **kwargs) Parameters steps : 步骤：列表(list) 被连接的（名称，变换）元组（实现拟合/变换）的列表，按照它们被连接的顺序，最后一个对象是估计器(estimator)。 memory: 内存参数,Instance of sklearn.external.joblib.Memory or string, optional (default=None) 属性, name_steps:bunch object，具有属性访问权限的字典 只读属性以用户给定的名称访问任何步骤参数。键是步骤名称，值是步骤参数。或者也可以直接通过”.步骤名称”获取 funcution Pipline的方法都是执行各个学习器中对应的方法,如果该学习器没有该方法,会报错 假设该Pipline共有n个学习器 transform ,依次执行各个学习器的transform方法 fit:依次对前n-1个学习器执行fit和transform方法,第n个学习器(最后一个学习器)执行fit方法 predict: 执行第n个学习器的predict方法 score： 执行第 n 个学习器的score方法 set_params: 设置第n个学习器的参数 get_param :,获取第n个学习器的参数 举例 问题是要对数据集 Breast Cancer Wisconsin 进行分类， 它包含 569 个样本，第一列 ID，第二列类别(M=恶性肿瘤，B=良性肿瘤)，第 3-32 列是实数值的特征。 # 加载数据 from sklearn.preprocessing import LabelEncoder df = pd.read_csv( \"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header=None) # 做基本的数据预处理 X = df.iloc[:, 2:].values y = df.iloc[:, 1].values le = LabelEncoder() # 将M-B等字符串编码成计算机能识别的0-1 y = le.fit_transform(y) le.transform(['M', 'B']) # 数据切分8：2 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=1) 我们要用 Pipeline 对训练集和测试集进行如下操作： 先用 StandardScaler 对数据集每一列做标准化处理，（是 transformer） 再用 PCA 将原始的 30 维度特征压缩的 2 维度，（是 transformer） 最后再用模型 LogisticRegression。（是 Estimator） 调用 Pipeline 时，输入由元组构成的列表，每个元组第一个值为变量名，元组第二个元素是 sklearn 中的 transformer 或 Estimator。 注意中间每一步是 transformer，即它们必须包含 fit 和 transform 方法，或者 fit_transform。 最后一步是一个 Estimator，即最后一步模型要有 fit 方法，可以没有 transform 方法。 然后用 Pipeline.fit对训练集进行训练，pipe_lr.fit(X_train, y_train) 再直接用 Pipeline.score 对测试集进行预测并评分 pipe_lr.score(X_test, y_test) 把所有的操作全部封在一个管道pipeline内形成一个工作流： 标准化+PCA+逻辑回归 # Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline pipe_lr2 = Pipeline([['std', StandardScaler()], ['pca', PCA(n_components=2)], [ 'lr', LogisticRegression(random_state=1)]]) pipe_lr2.fit(X_train, y_train) y_pred2 = pipe_lr2.predict(X_test) print(\"Test Accuracy: %.3f\" % pipe_lr2.score(X_test, y_test)) 结果：Test Accuracy: 0.956 Pipeline 的工作方式： 当管道 Pipeline 执行 fit 方法时， 首先 StandardScaler 执行 fit和 transform 方法， 然后将转换后的数据输入给 PCA， PCA 同样执行 fit 和 transform 方法， 再将数据输入给 LogisticRegression，进行训练。 注意中间每一步是transformer，即它们必须包含 fit 和 transform 方法，或者fit_transform。 最后一步是一个Estimator，即最后一步模型要有 fit 方法，可以没有 transform 方法。 当然，还可以用来选择特征，也可以应用 K-fold cross validation 与交叉验证结合 k 折交叉验证 from sklearn.model_selection import cross_val_score scores1 = cross_val_score(estimator=pipe_lr1, X=X_train, y=y_train, cv=10, n_jobs=1) print(\"CV accuracy scores:%s\" % scores1) print(\"CV accuracy:%.3f +/-%.3f\" % (np.mean(scores1), np.std(scores1))) 结果： CV accuracy scores:[0.93478261 0.93478261 0.95652174 0.95652174 0.93478261 0.95555556 0.97777778 0.93333333 0.95555556 0.95555556] CV accuracy:0.950 +/-0.014 分层 k 折交叉验证 # 分层 k折交叉验证 from sklearn.model_selection import StratifiedKFold kfold = StratifiedKFold(n_splits=10, random_state=1).split(X_train, y_train) scores2 = [] for k, (train, test) in enumerate(kfold): pipe_lr1.fit(X_train[train], y_train[train]) score = pipe_lr1.score(X_train[test], y_train[test]) scores2.append(score) print('Fold:%2d,Class dist.:%s,Acc:%.3f' % (k+1, np.bincount(y_train[train]), score)) print('\\nCV accuracy :%.3f +/-%.3f' % (np.mean(scores2), np.std(scores2))) 结果： Fold: 1,Class dist.:[256 153],Acc:0.935 Fold: 2,Class dist.:[256 153],Acc:0.935 Fold: 3,Class dist.:[256 153],Acc:0.957 Fold: 4,Class dist.:[256 153],Acc:0.957 Fold: 5,Class dist.:[256 153],Acc:0.935 Fold: 6,Class dist.:[257 153],Acc:0.956 Fold: 7,Class dist.:[257 153],Acc:0.978 Fold: 8,Class dist.:[257 153],Acc:0.933 Fold: 9,Class dist.:[257 153],Acc:0.956 Fold:10,Class dist.:[257 153],Acc:0.956 CV accuracy :0.950 +/-0.014 参考 Datawhale:常用数据分析方法：方差分析及实现！ 利用sklearn中pipeline构建机器学习工作流 sklearn.pipeline.Pipeline Update time： 2020-07-08 "},"pipeline/make_pipeline.html":{"url":"pipeline/make_pipeline.html","title":"make_pipeline","keywords":"","body":"make_pipeline 用 Pipeline类构建管道时语法有点麻烦，我们通常不需要为每一个步骤提供用户指定的名称，这种情况下，就可以用make_pipeline 函数创建管道，它可以为我们创建管道并根据每个步骤所属的类为其自动命名。 from sklearn.pipeline import make_pipeline pipe = make_pipeline(MinMaxScaler(),SVC()) 一般来说，自动命名的步骤名称是类名称的小写版本，如果多个步骤属于同一个类，则会附加一个数字。 案例 # 加载数据 from sklearn.preprocessing import LabelEncoder df = pd.read_csv( \"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header=None) # 做基本的数据预处理 X = df.iloc[:, 2:].values y = df.iloc[:, 1].values le = LabelEncoder() # 将M-B等字符串编码成计算机能识别的0-1 y = le.fit_transform(y) le.transform(['M', 'B']) # 数据切分8：2 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=1) # 把所有的操作全部封在一个管道pipeline内形成一个工作流： # 标准化+PCA+逻辑回归 # make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline pipe_lr1 = make_pipeline(StandardScaler(), PCA( n_components=2), LogisticRegression(random_state=1) ) pipe_lr1.fit(X_train, y_train) y_pred1 = pipe_lr1.predict(X_test) print(\"Test Accuracy: %.3f\" % pipe_lr1.score(X_test, y_test)) 结果： Test Accuracy: 0.956 make_pipeline 同样可以和交叉验证等相结合 。 参考 Datawhale:常用数据分析方法：方差分析及实现！ 《Python机器学习基础教程》构建管道(make_pipeline) Update time： 2020-07-08 "},"compose/":{"url":"compose/","title":"compose","keywords":"","body":"compose Update time： 2020-07-15 "},"compose/ColumnTransformer.html":{"url":"compose/ColumnTransformer.html","title":"ColumnTransformer","keywords":"","body":"ColumnTransformer Update time： 2020-07-15 "}}